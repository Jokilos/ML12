{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0x1m0s-3jdQp"
      },
      "source": [
        "# Lab 08: Imitation Learning\n",
        "\n",
        "In this lab, we look into the problem of learning from expert demonstrations.\n",
        "\n",
        "- Find a policy $\\pi(a | s)$ that best imitates the expert policy $\\pi^*(a | s)$ in the given environment.\n",
        "- It's worth noting, that we don't need access to the environment rewards.\n",
        "\n",
        "Major Imitation Learning techniques are:\n",
        "\n",
        "1. Behavioural Cloning,\n",
        "1. Imitation Learning via Interactive Demonstrator e.g. SMILe (Ross and Bagnell, 2010) or DAgger (Ross et al., 2011),\n",
        "1. Inverse Reinforcement Learning -- out of scope of this lab.\n",
        "\n",
        "We will solve the Ant problem, shown below, examining the first two approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjqgLL8EjjAh"
      },
      "source": [
        "## Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Yt4UPVjXjZKI"
      },
      "outputs": [],
      "source": [
        "!pip -q install gymnasium[mujoco]\n",
        "!pip -q install pyglet\n",
        "!pip -q install pyopengl\n",
        "!pip -q install pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEJxM_OWyIiu",
        "outputId": "71cf9575-bd44-4090-ed19-2f03d4a199cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy==1.23.5 in /usr/local/lib/python3.11/dist-packages (1.23.5)\n",
            "Requirement already satisfied: torch==2.5 in /usr/local/lib/python3.11/dist-packages (2.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.5) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.5) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.5) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.5) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.5) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.5) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.5) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.5) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.5) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.5) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.5) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.5) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.5) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.5) (3.0.2)\n",
            "fatal: destination path 'sample-factory' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy==1.23.5\n",
        "!pip install torch==2.5\n",
        "!git clone https://github.com/lychanl/sample-factory.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZL76yoD36R2",
        "outputId": "acb8a135-6de0-41ff-ae15-e80abb084e34"
      },
      "outputs": [],
      "source": [
        "!pip install -q sample-factory[mujoco]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jjfRlCiT4SOV",
        "outputId": "b10fb091-4bba-42e1-90ac-d01d63523a6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/ML-stuff/1.2/RL/lab08/sample-factory\n"
          ]
        }
      ],
      "source": [
        "%cd sample-factory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iGPfs9zjpkQ"
      },
      "source": [
        "## Download Expert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Urgn_w9OyKJ-",
        "outputId": "c23af7f6-ba49-441b-9607-aa0458543d6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
            "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
            "  warnings.warn(warning_message, FutureWarning)\n",
            "/content/ML-stuff/1.2/RL/lab08/sample-factory/./train_dir/sf_Ant is already a clone of https://huggingface.co/LLParallax/sf_Ant. Make sure you pull the latest changes with `repo.git_pull()`.\n",
            "\u001b[37m\u001b[1m[2025-04-24 16:55:00,826][11904] The repository LLParallax/sf_Ant has been cloned to ./train_dir/sf_Ant\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python -m sample_factory.huggingface.load_from_hub -r LLParallax/sf_Ant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jAKmX9wQmJKM"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "\n",
        "import torch\n",
        "\n",
        "from sample_factory.algo.learning.learner import Learner\n",
        "from sample_factory.algo.utils.env_info import extract_env_info\n",
        "from sample_factory.algo.utils.make_env import make_env_func_batched\n",
        "from sample_factory.algo.utils.rl_utils import prepare_and_normalize_obs\n",
        "from sample_factory.cfg.arguments import load_from_checkpoint\n",
        "from sample_factory.model.actor_critic import create_actor_critic\n",
        "from sample_factory.model.model_utils import get_rnn_size\n",
        "from sample_factory.utils.attr_dict import AttrDict\n",
        "from sample_factory.utils.typing import Config\n",
        "\n",
        "\n",
        "def create_expert(cfg):\n",
        "    cfg = load_from_checkpoint(cfg)\n",
        "\n",
        "    cfg.num_envs = 1\n",
        "\n",
        "    env = make_env_func_batched(\n",
        "        cfg, env_config=AttrDict(worker_index=0, vector_index=0, env_id=0), render_mode=None\n",
        "    )\n",
        "\n",
        "    if hasattr(env.unwrapped, \"reset_on_init\"):\n",
        "        # reset call ruins the demo recording for VizDoom\n",
        "        env.unwrapped.reset_on_init = False\n",
        "\n",
        "    actor_critic = create_actor_critic(cfg, env.observation_space, env.action_space)\n",
        "    actor_critic.eval()\n",
        "\n",
        "    device = torch.device(\"cpu\" if cfg.device == \"cpu\" else \"cuda\")\n",
        "    actor_critic.model_to_device(device)\n",
        "\n",
        "    policy_id = cfg.policy_index\n",
        "    name_prefix = dict(latest=\"checkpoint\", best=\"best\")[cfg.load_checkpoint_kind]\n",
        "    checkpoints = Learner.get_checkpoints(Learner.checkpoint_dir(cfg, policy_id), f\"{name_prefix}_*\")\n",
        "    checkpoint_dict = Learner.load_checkpoint(checkpoints, device)\n",
        "    actor_critic.load_state_dict(checkpoint_dict[\"model\"])\n",
        "    return actor_critic\n",
        "\n",
        "\n",
        "def get_expert_actions(obs, cfg: Config, actor_critic, env, env_info, device):\n",
        "    rnn_states = torch.zeros([env.num_agents, get_rnn_size(cfg)], dtype=torch.float32, device=device)\n",
        "\n",
        "    obs = {\"obs\": obs}\n",
        "    with torch.no_grad():\n",
        "        normalized_obs = prepare_and_normalize_obs(actor_critic, obs)\n",
        "        policy_outputs = actor_critic(normalized_obs, rnn_states)\n",
        "\n",
        "        # sample actions from the distribution by default\n",
        "        actions = policy_outputs[\"actions\"]\n",
        "    return actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2naspguhjm9h"
      },
      "source": [
        "## Load expert model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoaZGg2kiYWl",
        "outputId": "2cca94c0-ad53-412b-ba7b-d5de78ca9acc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[33m[2025-04-24 16:55:04,034][11684] Loading existing experiment configuration from train_dir/sf_Ant/config.json\u001b[0m\n",
            "\u001b[36m[2025-04-24 16:55:04,036][11684] Overriding arg 'experiment' with value 'sf_Ant' passed from command line\u001b[0m\n",
            "\u001b[36m[2025-04-24 16:55:04,037][11684] Overriding arg 'train_dir' with value 'train_dir' passed from command line\u001b[0m\n",
            "\u001b[36m[2025-04-24 16:55:04,038][11684] Adding new argument 'wandb_dir'='/content/ML-stuff/1.2/RL/lab08/sample-factory/wandb' that is not in the saved config file!\u001b[0m\n",
            "\u001b[36m[2025-04-24 16:55:04,039][11684] Adding new argument 'fps'=0 that is not in the saved config file!\u001b[0m\n",
            "\u001b[36m[2025-04-24 16:55:04,039][11684] Adding new argument 'eval_env_frameskip'=None that is not in the saved config file!\u001b[0m\n",
            "\u001b[36m[2025-04-24 16:55:04,040][11684] Adding new argument 'no_render'=True that is not in the saved config file!\u001b[0m\n",
            "\u001b[36m[2025-04-24 16:55:04,040][11684] Adding new argument 'save_video'=False that is not in the saved config file!\u001b[0m\n",
            "\u001b[36m[2025-04-24 16:55:04,041][11684] Adding new argument 'video_frames'=1000000000.0 that is not in the saved config file!\u001b[0m\n",
            "\u001b[36m[2025-04-24 16:55:04,042][11684] Adding new argument 'video_name'=None that is not in the saved config file!\u001b[0m\n",
            "\u001b[36m[2025-04-24 16:55:04,043][11684] Adding new argument 'max_num_frames'=1000000000.0 that is not in the saved config file!\u001b[0m\n",
            "\u001b[36m[2025-04-24 16:55:04,044][11684] Adding new argument 'max_num_episodes'=1000000000.0 that is not in the saved config file!\u001b[0m\n",
            "\u001b[36m[2025-04-24 16:55:04,044][11684] Adding new argument 'push_to_hub'=False that is not in the saved config file!\u001b[0m\n",
            "\u001b[36m[2025-04-24 16:55:04,045][11684] Adding new argument 'hf_repository'=None that is not in the saved config file!\u001b[0m\n",
            "\u001b[36m[2025-04-24 16:55:04,046][11684] Adding new argument 'policy_index'=0 that is not in the saved config file!\u001b[0m\n",
            "\u001b[36m[2025-04-24 16:55:04,047][11684] Adding new argument 'eval_deterministic'=False that is not in the saved config file!\u001b[0m\n",
            "\u001b[36m[2025-04-24 16:55:04,047][11684] Adding new argument 'train_script'=None that is not in the saved config file!\u001b[0m\n",
            "\u001b[36m[2025-04-24 16:55:04,048][11684] Adding new argument 'enjoy_script'=None that is not in the saved config file!\u001b[0m\n",
            "\u001b[36m[2025-04-24 16:55:04,049][11684] Adding new argument 'sample_env_episodes'=256 that is not in the saved config file!\u001b[0m\n",
            "\u001b[36m[2025-04-24 16:55:04,049][11684] Adding new argument 'csv_folder_name'=None that is not in the saved config file!\u001b[0m\n",
            "/usr/local/lib/python3.11/dist-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.num_agents to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.num_agents` for environment variables or `env.get_wrapper_attr('num_agents')` that will search the reminding wrappers.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_multiagent to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_multiagent` for environment variables or `env.get_wrapper_attr('is_multiagent')` that will search the reminding wrappers.\u001b[0m\n",
            "  logger.warn(\n",
            "\u001b[36m[2025-04-24 16:55:04,215][11684] RunningMeanStd input shape: (27,)\u001b[0m\n",
            "\u001b[36m[2025-04-24 16:55:04,217][11684] RunningMeanStd input shape: (1,)\u001b[0m\n",
            "\u001b[33m[2025-04-24 16:55:04,563][11684] Loading state from checkpoint train_dir/sf_Ant/checkpoint_p0/checkpoint_000019544_10006528.pth...\u001b[0m\n",
            "/content/ML-stuff/1.2/RL/lab08/sample-factory/sample_factory/algo/learning/learner.py:281: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint_dict = torch.load(latest_checkpoint, map_location=device)\n"
          ]
        }
      ],
      "source": [
        "from sample_factory.cfg.arguments import parse_full_cfg, parse_sf_args\n",
        "from sample_factory.envs.env_utils import register_env\n",
        "from sf_examples.mujoco.mujoco_params import add_mujoco_env_args, mujoco_override_defaults\n",
        "from sf_examples.mujoco.train_mujoco import register_mujoco_components\n",
        "from sf_examples.mujoco.mujoco_utils import MUJOCO_ENVS, make_mujoco_env\n",
        "\n",
        "\n",
        "def register_mujoco_components():\n",
        "    for env in MUJOCO_ENVS:\n",
        "        register_env(env.name, make_mujoco_env)\n",
        "\n",
        "\n",
        "register_mujoco_components()\n",
        "argv = [\"--algo=APPO\", \"--env=mujoco_ant\", \"--experiment=sf_Ant\", \"--train_dir=train_dir\", \"--no_render\"]\n",
        "parser, partial_cfg = parse_sf_args(argv=argv, evaluation=True)\n",
        "add_mujoco_env_args(partial_cfg.env, parser)\n",
        "mujoco_override_defaults(partial_cfg.env, parser)\n",
        "cfg = parse_full_cfg(parser, argv=argv)\n",
        "expert = create_expert(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrGdCIr0iaLt"
      },
      "source": [
        "## Helpers\n",
        "collecting data  \n",
        "\n",
        "evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OPi_r6R8ibVF"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "from IPython import display as ipydisplay\n",
        "\n",
        "import torch\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from matplotlib import animation\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def run_policy (env, model, total_steps=10000, verbose=True):\n",
        "    obs_array = np.empty([total_steps, *env.observation_space.shape])\n",
        "    act_array = np.empty([total_steps, env.action_space.shape[0]])\n",
        "    rew_array = np.empty([total_steps, 1])\n",
        "    done_array = np.empty([total_steps, 1])\n",
        "\n",
        "    iter_time = time.time()\n",
        "    done = True\n",
        "    for i in range(total_steps):\n",
        "        if verbose and (i + 1) % 1000 == 0:\n",
        "            steps_per_second = 1000 / (time.time() - iter_time)\n",
        "            print(f'Step {i + 1}/{total_steps}, Steps per second: {steps_per_second}')\n",
        "            iter_time = time.time()\n",
        "\n",
        "        if done:\n",
        "            obs, info = env.reset()\n",
        "\n",
        "        act = model(torch.from_numpy(obs).unsqueeze(0).float())[0].detach().cpu().numpy()\n",
        "        obs_, rew, terminated, truncated, _ = env.step(act)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        obs_array[i] = obs\n",
        "        act_array[i] = act\n",
        "        rew_array[i] = rew\n",
        "        done_array[i] = float(done)\n",
        "\n",
        "        obs = obs_\n",
        "\n",
        "    return obs_array, act_array, rew_array, done_array\n",
        "\n",
        "def calculate_returns(rew, done):\n",
        "    rew_cumsum = np.cumsum(rew)[:, None]\n",
        "    ret_cumsum = rew_cumsum * done\n",
        "    ret_cumsum_trimed = ret_cumsum[np.nonzero(ret_cumsum)]\n",
        "    ret_cumsum_trimed[1:] -= ret_cumsum_trimed[:-1]\n",
        "    return ret_cumsum_trimed\n",
        "\n",
        "def evaluate_agent(env, model, verbose=False):\n",
        "    _, _, rew, done = run_policy(env, model, total_steps=50000, verbose=verbose)\n",
        "    rets = calculate_returns(rew, done)\n",
        "\n",
        "    print(f'Num. episodes: {len(rets)}')\n",
        "    print(f'Avg. return: {np.mean(rets)}')\n",
        "    print(f'Max. return: {np.max(rets)}')\n",
        "    print(f'Min. return: {np.min(rets)}')\n",
        "\n",
        "@torch.no_grad()\n",
        "def collect_frames(eval_env, model, num_frames=2000):\n",
        "    state, _ = eval_env.reset()\n",
        "    state = torch.from_numpy(np.array(state)).float()\n",
        "    frames = []\n",
        "\n",
        "    for _ in range(num_frames):\n",
        "        frames.append(eval_env.render())\n",
        "\n",
        "        action = model(state.unsqueeze(0))[0]\n",
        "        next_state, reward, terminal, truncate, info = eval_env.step(action.detach().cpu().numpy())\n",
        "\n",
        "        if terminal or truncate:\n",
        "            state, _ = eval_env.reset()\n",
        "        state = next_state\n",
        "        state = torch.from_numpy(np.array(state)).float()\n",
        "\n",
        "    return frames\n",
        "\n",
        "def display_frames_as_video(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a video.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi=72)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=50)\n",
        "    ipydisplay.display(ipydisplay.HTML(anim.to_jshtml()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSiZuQnzijMY"
      },
      "source": [
        "## 1. Behavior Clonning\n",
        "\n",
        "Algorithm\n",
        "\n",
        "1. Collect the expert data.\n",
        "2. Fit the model (classifier/regressor) to the expert data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGkOlSfWj5Cs"
      },
      "source": [
        "### Create model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "v8e3RXYsij0O"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_shape, output_size, hidden_sizes=(256, 256), hidden_activation=nn.Tanh(), output_activation=None, l2_weight=0.0001):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layers = nn.Sequential()\n",
        "\n",
        "        # Input layer\n",
        "        self.layers.add_module(\"input\", nn.Linear(input_shape, hidden_sizes[0]))\n",
        "        self.layers.add_module(\"input_activation\", hidden_activation)\n",
        "\n",
        "        # Hidden layers\n",
        "        layer_sizes = zip(hidden_sizes[:-1], hidden_sizes[1:])\n",
        "        for i, (h1, h2) in enumerate(layer_sizes):\n",
        "            self.layers.add_module(f\"hidden_{i}\", nn.Linear(h1, h2))\n",
        "            self.layers.add_module(f\"activation_{i}\", hidden_activation)\n",
        "\n",
        "        # Output layer\n",
        "        self.layers.add_module(\"output\", nn.Linear(hidden_sizes[-1], output_size))\n",
        "        if output_activation is not None:\n",
        "            self.layers.add_module(\"output_activation\", output_activation)\n",
        "\n",
        "        # Regularization\n",
        "        self.l2_weight = l2_weight\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through the network\n",
        "        x = self.layers(x)\n",
        "        return x\n",
        "\n",
        "    def l2_regularization(self):\n",
        "        l2_reg = None\n",
        "        for name, param in self.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                if l2_reg is None:\n",
        "                    l2_reg = param.norm(2)\n",
        "                else:\n",
        "                    l2_reg = l2_reg + param.norm(2)\n",
        "        return self.l2_weight * l2_reg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmDBOLRIilgz"
      },
      "source": [
        "### Function for training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ft4CtoNgik6E"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "\n",
        "def train(obs, act, model, num_epochs=10, batch_size=32):\n",
        "    obs_tensor = torch.tensor(obs, dtype=torch.float32)\n",
        "    act_tensor = torch.tensor(act, dtype=torch.float32)\n",
        "\n",
        "    dataset = TensorDataset(obs_tensor, act_tensor)\n",
        "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "    # Define the loss function and optimizer\n",
        "    loss_fn = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        for batch_idx, (x_batch, y_batch) in enumerate(data_loader):\n",
        "            # Forward pass\n",
        "            y_pred = model(x_batch)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = loss_fn(y_pred, y_batch) + model.l2_regularization()\n",
        "\n",
        "            # Zero gradients, perform a backward pass, and update the weights.\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Print loss every epoch\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cQA_gW6iofY",
        "outputId": "f7a079b4-9688-463c-8aa6-77926084f609"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1000/10000, Steps per second: 369.21153874085456\n",
            "Step 2000/10000, Steps per second: 554.943421439983\n",
            "Step 3000/10000, Steps per second: 578.2246508777101\n",
            "Step 4000/10000, Steps per second: 561.3150039037606\n",
            "Step 5000/10000, Steps per second: 541.2807653676655\n",
            "Step 6000/10000, Steps per second: 555.0655522753391\n",
            "Step 7000/10000, Steps per second: 437.14705200912044\n",
            "Step 8000/10000, Steps per second: 417.0037301928885\n",
            "Step 9000/10000, Steps per second: 530.0576108367568\n",
            "Step 10000/10000, Steps per second: 565.5538486493964\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('Ant-v4')\n",
        "env.num_agents = 1\n",
        "env_info = extract_env_info(env, cfg)\n",
        "device = torch.device(\"cpu\" if cfg.device == \"cpu\" else \"cuda\")\n",
        "collected_data = run_policy(env, functools.partial(get_expert_actions, cfg=cfg, actor_critic=expert, env=env, env_info=env_info, device=device), total_steps=10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTsFAoG4iqN4",
        "outputId": "1e3cd109-b01c-4345-97c8-844ed02b250c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10000, 27) (10000, 8) (10000, 1) (10000, 1)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 0.01409242581576109\n",
            "Epoch 2/10, Loss: 0.012592637911438942\n",
            "Epoch 3/10, Loss: 0.01511292066425085\n",
            "Epoch 4/10, Loss: 0.009069368243217468\n",
            "Epoch 5/10, Loss: 0.012666529044508934\n",
            "Epoch 6/10, Loss: 0.010790582746267319\n",
            "Epoch 7/10, Loss: 0.007859050296247005\n",
            "Epoch 8/10, Loss: 0.010145654901862144\n",
            "Epoch 9/10, Loss: 0.01104153972119093\n",
            "Epoch 10/10, Loss: 0.012880825437605381\n"
          ]
        }
      ],
      "source": [
        "obs, act, rewards, dones = collected_data\n",
        "\n",
        "print(obs.shape, act.shape, rewards.shape, dones.shape)\n",
        "\n",
        "# print(obs)\n",
        "\n",
        "# EXERCISE: Create model\n",
        "model = MLP(obs.shape[1], act.shape[1])\n",
        "\n",
        "train(obs, act, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCjtebL4irnz",
        "outputId": "e986d682-61f5-4654-de5a-1630f0ab77f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num. episodes: 66\n",
            "Avg. return: 3749.199646308135\n",
            "Max. return: 5834.928417044925\n",
            "Min. return: 20.85465267905238\n"
          ]
        }
      ],
      "source": [
        "evaluate_agent(env, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgaDRcOgisq5"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "Discuss the questions\n",
        "\n",
        "1. In principle, do we need the expert policy for BC?\n",
        "\n",
        "2. What are the problems with BC?\n",
        "\n",
        "3. How can we help BC do better?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0Z_UzUBithV",
        "outputId": "74177c66-344e-4f8d-93fc-a69b14178d5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1000/10000, Steps per second: 550.3415110988647\n",
            "Step 2000/10000, Steps per second: 532.1157553333\n",
            "Step 3000/10000, Steps per second: 411.6771605325425\n",
            "Step 4000/10000, Steps per second: 428.5263867913097\n",
            "Step 5000/10000, Steps per second: 547.7728002064513\n",
            "Step 6000/10000, Steps per second: 541.01133668098\n",
            "Step 7000/10000, Steps per second: 546.9134683653571\n",
            "Step 8000/10000, Steps per second: 559.8663145753446\n",
            "Step 9000/10000, Steps per second: 544.922158691554\n",
            "Step 10000/10000, Steps per second: 434.3872657738518\n"
          ]
        }
      ],
      "source": [
        "# Collect the exploratory data\n",
        "def exploratory(obs, **kwargs):\n",
        "    \"\"\"Adds the Gaussian noise to the expert actions.\"\"\"\n",
        "    rnn_states = torch.zeros([env.num_agents, get_rnn_size(cfg)], dtype=torch.float32, device=device)\n",
        "\n",
        "    obs = {\"obs\": obs}\n",
        "    with torch.no_grad():\n",
        "        normalized_obs = prepare_and_normalize_obs(kwargs['actor_critic'], obs)\n",
        "        policy_outputs = kwargs['actor_critic'](normalized_obs, rnn_states)\n",
        "\n",
        "        # sample actions from the distribitution by default\n",
        "        actions = policy_outputs[\"actions\"]\n",
        "\n",
        "    actions += torch.randn_like(actions) / 10\n",
        "    return actions\n",
        "\n",
        "expl_data = run_policy(env, functools.partial(exploratory, cfg=cfg, actor_critic=expert, env=env, env_info=env_info, device=device), total_steps=10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMgZIS9piuf5",
        "outputId": "a22e9d4e-1c8c-4014-c5b9-7a27a0f11a52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 0.03041917271912098\n",
            "Epoch 2/10, Loss: 0.02428119257092476\n",
            "Epoch 3/10, Loss: 0.0363578125834465\n",
            "Epoch 4/10, Loss: 0.025699084624648094\n",
            "Epoch 5/10, Loss: 0.027162104845046997\n",
            "Epoch 6/10, Loss: 0.027334270998835564\n",
            "Epoch 7/10, Loss: 0.0225650817155838\n",
            "Epoch 8/10, Loss: 0.019606370478868484\n",
            "Epoch 9/10, Loss: 0.02227487787604332\n",
            "Epoch 10/10, Loss: 0.02308768220245838\n"
          ]
        }
      ],
      "source": [
        "obs_expl, act_expl, rewards, dones = expl_data\n",
        "# Exercise: Run BC on the exploratory data\n",
        "\n",
        "# ANSWER\n",
        "model_expl = MLP(obs_expl.shape[1], act_expl.shape[1])\n",
        "\n",
        "train(obs_expl, act_expl, model_expl)\n",
        "# END ANSWER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bgu4_EG1ivJp",
        "outputId": "cc3cc88d-fd8c-4b2a-cdfc-4777d8c618f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num. episodes: 54\n",
            "Avg. return: 4755.789993483126\n",
            "Max. return: 5639.069530215638\n",
            "Min. return: 48.74341965483836\n"
          ]
        }
      ],
      "source": [
        "evaluate_agent(env, model_expl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nif8vM9xiv1I"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "Answer the questions\n",
        "\n",
        "1. Why does it do better?\n",
        "\n",
        "2. How can we use the expert to further improve the data?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "NkJSTo7CiwkJ",
        "outputId": "95224164-5526-4f10-b521-897c0731062a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1000/10000, Steps per second: 396.4603215368312\n",
            "Step 2000/10000, Steps per second: 422.40114726874106\n",
            "Step 3000/10000, Steps per second: 574.4097764743793\n",
            "Step 4000/10000, Steps per second: 567.0541616446894\n",
            "Step 5000/10000, Steps per second: 567.7668777927734\n",
            "Step 6000/10000, Steps per second: 577.6359270666994\n",
            "Step 7000/10000, Steps per second: 571.7701519715985\n",
            "Step 8000/10000, Steps per second: 490.80509253533006\n",
            "Step 9000/10000, Steps per second: 428.8574150401889\n",
            "Step 10000/10000, Steps per second: 478.4076317025538\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-d020b5d53d45>:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  obs_tensor = torch.tensor(obs, dtype=torch.float32)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 0.01786557212471962\n",
            "Epoch 2/10, Loss: 0.020376749336719513\n",
            "Epoch 3/10, Loss: 0.01852814108133316\n",
            "Epoch 4/10, Loss: 0.018537839874625206\n",
            "Epoch 5/10, Loss: 0.01323208212852478\n",
            "Epoch 6/10, Loss: 0.01210375688970089\n",
            "Epoch 7/10, Loss: 0.014186827465891838\n",
            "Epoch 8/10, Loss: 0.010674932971596718\n",
            "Epoch 9/10, Loss: 0.012855072505772114\n",
            "Epoch 10/10, Loss: 0.011490416713058949\n"
          ]
        }
      ],
      "source": [
        "# Exercise: Infere the expert actions on the exploratory observations\n",
        "#           and run BC on it.\n",
        "\n",
        "collected_data = run_policy(env, functools.partial(get_expert_actions, cfg=cfg, actor_critic=expert, env=env, env_info=env_info, device=device), total_steps=10000)\n",
        "\n",
        "obs, act, rewards, dones = collected_data\n",
        "\n",
        "obs = torch.tensor(obs, dtype=torch.float32)\n",
        "eobs = obs + torch.randn_like(obs) / 10\n",
        "\n",
        "model_oexpl = MLP(eobs.shape[1], act.shape[1])\n",
        "\n",
        "train(eobs, act, model_oexpl)\n",
        "# ANSWER\n",
        "# ANSWER END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpiX-6KxixSp",
        "outputId": "0368885f-1f29-4987-d271-15cca6efcbc0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num. episodes: 58\n",
            "Avg. return: 2647.846824096608\n",
            "Max. return: 4700.022542569983\n",
            "Min. return: 63.91135489550652\n"
          ]
        }
      ],
      "source": [
        "evaluate_agent(env, model_oexpl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81NLiQFqiyHd"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "Answer the questions\n",
        "\n",
        "1. Did it help? Why?\n",
        "\n",
        "\n",
        "1. How can you extend this idea?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wiOVC31Qiy8q"
      },
      "source": [
        "## 2. Imitation Learning via Interactive Demostrator\n",
        "\n",
        "[DAgger](https://www.ri.cmu.edu/pub_files/2011/4/Ross-AISTATS11-NoRegret.pdf)\n",
        "\n",
        "1. Collect the expert data.\n",
        "2. Fit the model (classifier/regressor) to the expert data.\n",
        "3. Collect the imitator data.\n",
        "4. Infere the expert actions on the imitator data.\n",
        "5. Fit the model to the extended dataset.\n",
        "6. Repeat from 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvy85dEoizye",
        "outputId": "0dc44432-9ebd-4643-d49d-5bcc13acdaa1"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model_dagger' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-81192204cbe1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# END ANSWER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mevaluate_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model_dagger' is not defined"
          ]
        }
      ],
      "source": [
        "# We will pre-train on less expert data to keep the same dataset size\n",
        "obs_ = obs[:2000, :]\n",
        "act_ = act[:2000, :]\n",
        "\n",
        "# EXERCISE: pretrain on first 2000 samples\n",
        "# ANSWER\n",
        "...\n",
        "# END ANSWER\n",
        "\n",
        "evaluate_agent(env, model_dagger)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uz_AQ9t3i0r3",
        "outputId": "df004be8-4092-4d7b-80d4-591021a188a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "### Iter. 1 ###\n",
            "\n",
            "1. Data collection\n",
            "Step 1000/2000, Steps per second: 3294.7693632223873\n",
            "Step 2000/2000, Steps per second: 3707.6622117245215\n",
            "\n",
            "2. Training\n",
            "Epoch 1/10, Loss: 0.01619561016559601\n",
            "Epoch 2/10, Loss: 0.01852767914533615\n",
            "Epoch 3/10, Loss: 0.013723315671086311\n",
            "Epoch 4/10, Loss: 0.014866928569972515\n",
            "Epoch 5/10, Loss: 0.014802966266870499\n",
            "Epoch 6/10, Loss: 0.011287961155176163\n",
            "Epoch 7/10, Loss: 0.015429697930812836\n",
            "Epoch 8/10, Loss: 0.01600058376789093\n",
            "Epoch 9/10, Loss: 0.012179029174149036\n",
            "Epoch 10/10, Loss: 0.011647832579910755\n",
            "\n",
            "3. Evaluation\n",
            "Num. episodes: 53\n",
            "Avg. return: 4466.662482786521\n",
            "Max. return: 5459.026620695091\n",
            "Min. return: 384.4503018264944\n",
            "\n",
            "### Iter. 2 ###\n",
            "\n",
            "1. Data collection\n",
            "Step 1000/2000, Steps per second: 3451.289655274561\n",
            "Step 2000/2000, Steps per second: 3548.693703259671\n",
            "\n",
            "2. Training\n",
            "Epoch 1/10, Loss: 0.01004914939403534\n",
            "Epoch 2/10, Loss: 0.010697782039642334\n",
            "Epoch 3/10, Loss: 0.009375996887683868\n",
            "Epoch 4/10, Loss: 0.00921697448939085\n",
            "Epoch 5/10, Loss: 0.009970655664801598\n",
            "Epoch 6/10, Loss: 0.01175720989704132\n",
            "Epoch 7/10, Loss: 0.010534362867474556\n",
            "Epoch 8/10, Loss: 0.007622719742357731\n",
            "Epoch 9/10, Loss: 0.01044491957873106\n",
            "Epoch 10/10, Loss: 0.010127197951078415\n",
            "\n",
            "3. Evaluation\n",
            "Num. episodes: 57\n",
            "Avg. return: 4768.642094900372\n",
            "Max. return: 6164.805995355135\n",
            "Min. return: 70.24710121710086\n",
            "\n",
            "### Iter. 3 ###\n",
            "\n",
            "1. Data collection\n",
            "Step 1000/2000, Steps per second: 3666.737478352168\n",
            "Step 2000/2000, Steps per second: 3622.9127005241353\n",
            "\n",
            "2. Training\n",
            "Epoch 1/10, Loss: 0.007917053997516632\n",
            "Epoch 2/10, Loss: 0.009996404871344566\n",
            "Epoch 3/10, Loss: 0.009498683735728264\n",
            "Epoch 4/10, Loss: 0.008137105032801628\n",
            "Epoch 5/10, Loss: 0.009362755343317986\n",
            "Epoch 6/10, Loss: 0.008086050860583782\n",
            "Epoch 7/10, Loss: 0.010338788852095604\n",
            "Epoch 8/10, Loss: 0.007983406074345112\n",
            "Epoch 9/10, Loss: 0.007980067282915115\n",
            "Epoch 10/10, Loss: 0.009110121987760067\n",
            "\n",
            "3. Evaluation\n",
            "Num. episodes: 60\n",
            "Avg. return: 4500.10965494311\n",
            "Max. return: 6043.150063411229\n",
            "Min. return: 349.27362888259813\n",
            "\n",
            "### Iter. 4 ###\n",
            "\n",
            "1. Data collection\n",
            "Step 1000/2000, Steps per second: 3578.4218204776002\n",
            "Step 2000/2000, Steps per second: 3619.2987663866243\n",
            "\n",
            "2. Training\n",
            "Epoch 1/10, Loss: 0.007250199094414711\n",
            "Epoch 2/10, Loss: 0.008454334922134876\n",
            "Epoch 3/10, Loss: 0.009125428274273872\n",
            "Epoch 4/10, Loss: 0.008615853264927864\n",
            "Epoch 5/10, Loss: 0.007782396860420704\n",
            "Epoch 6/10, Loss: 0.008323581889271736\n",
            "Epoch 7/10, Loss: 0.007024532184004784\n",
            "Epoch 8/10, Loss: 0.0071644289419054985\n",
            "Epoch 9/10, Loss: 0.009446516633033752\n",
            "Epoch 10/10, Loss: 0.007878946140408516\n",
            "\n",
            "3. Evaluation\n",
            "Num. episodes: 55\n",
            "Avg. return: 5022.044324547806\n",
            "Max. return: 5987.444502523242\n",
            "Min. return: 106.32973088577273\n"
          ]
        }
      ],
      "source": [
        "# Exercise: Implement DAgger\n",
        "\n",
        "for i in range(4):\n",
        "    print(f'\\n### Iter. {i+1} ###')\n",
        "\n",
        "    # ANSWER\n",
        "    print('\\n1. Data collection')\n",
        "    obs_extra, _, _, _ = # Collect 2k steps\n",
        "\n",
        "\n",
        "    print('\\n2. Training')\n",
        "    # reset model for fair comparison\n",
        "    model_dagger = ...\n",
        "\n",
        "    # END ANSWER\n",
        "\n",
        "    print('\\n3. Evaluation')\n",
        "    evaluate_agent(env, model_dagger)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGKMEWQti3oo"
      },
      "source": [
        "### Note\n",
        "\n",
        "Training the expert with the PPO algorithm took 10M data samples (env. interactions). Here, we nearly match it with only 10k samples! Training from the expert can be much more efficient than reinforcement learning."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
