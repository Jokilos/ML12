{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdRZMpFw_66J"
      },
      "source": [
        "# Retrieval Augmented Generation (RAG)\n",
        "\n",
        "\n",
        "\n",
        "![RAG](https://miro.medium.com/v2/resize:fit:1200/0*zMIZgSKLG7uPb7eQ.png \"Retrieval Augmented Generation\")\n",
        "<small> Image source: https://miro.medium.com/v2/resize:fit:1200/0*zMIZgSKLG7uPb7eQ.png </small>\n",
        "\n",
        "In this scenario you are going to build a Retrieval Augmented Generation pipeline for the [question answering task](https://paperswithcode.com/task/question-answering). You will learn:\n",
        "* What are the parts of the RAG pipeline?\n",
        "* How to implement the pipeline using the [dspy](https://github.com/stanfordnlp/dspy) framework?\n",
        "* What can be done to automate the evaluation of your solution?\n",
        "\n",
        "The advent of Large Language Models (LLMs) is a fantastic progress in many well-known NLP tasks. One of such tasks is Question Answering. By and large, in Question Answering the input is a question in natural language and the expected output is the answer to that question.\n",
        "LLMs achieve very good results in zero-shot or few-shot scenarios, i.e., when the model is provided 0 or few annotated examples. However, due to the nature of text generation, LLMs sometimes output confident, but incorrect answers. This phenomenon is called [hallucination](https://arxiv.org/pdf/2311.05232.pdf).\n",
        "One of the ways to reduce model hallucination is to enrich the question with paragrphs of text containing the answer. The model is able to use the paragraph in prompt to output correct answer tailored to the use case needs. This technique is called the [Retrieval Augmented Generation](https://arxiv.org/pdf/2005.11401.pdf).\n",
        "While the technique can be used in the end-to-end training, due to time and resource contraints we are going to focus on the zero-shot variant based on pretrained retriever and generator. This approach is often a very good first candidate in real use cases.\n",
        "\n",
        "This laboratory scenario uses [dspy](https://github.com/stanfordnlp/dspy), a framework by Stanford NLP. Based on the documentation, \"DSPy is a framework for algorithmically optimizing LM prompts and weights, especially when LMs are used one or more times within a pipeline.\" The framework enables rapid development of pipelines based on pretrained models, and has an internal compiler aimed at optimization of itermediate prompts to maximize the desired result. In this laboratory we are going to focus on the construction of the dspy pipelines, but are not going to use the compiler. Please see [here](https://github.com/stanfordnlp/dspy?tab=readme-ov-file#4b-asking-dspy-to-automatically-optimize-your-program-with-dspyteleprompt) if you are interested in automated optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6QFqqY85_66L"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cohere in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (5.13.12)\n",
            "Requirement already satisfied: boto3 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (1.37.24)\n",
            "Requirement already satisfied: dspy in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (2.6.16)\n",
            "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from cohere) (1.10.0)\n",
            "Requirement already satisfied: httpx>=0.21.2 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from cohere) (0.28.1)\n",
            "Requirement already satisfied: httpx-sse==0.4.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from cohere) (0.4.0)\n",
            "Requirement already satisfied: pydantic>=1.9.2 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from cohere) (2.10.6)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from cohere) (2.27.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from cohere) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from cohere) (0.21.0)\n",
            "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from cohere) (2.32.0.20241016)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from cohere) (4.12.2)\n",
            "Requirement already satisfied: botocore<1.38.0,>=1.37.24 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from boto3) (1.37.24)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from boto3) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from boto3) (0.11.4)\n",
            "Requirement already satisfied: backoff>=2.2 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from dspy) (2.2.1)\n",
            "Requirement already satisfied: joblib~=1.3 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from dspy) (1.4.2)\n",
            "Requirement already satisfied: openai<=1.61.0,>=0.28.1 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from dspy) (1.61.0)\n",
            "Requirement already satisfied: pandas>=2.1.1 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from dspy) (2.2.3)\n",
            "Requirement already satisfied: regex>=2023.10.3 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from dspy) (2024.11.6)\n",
            "Requirement already satisfied: ujson>=5.8.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from dspy) (5.10.0)\n",
            "Requirement already satisfied: tqdm>=4.66.1 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from dspy) (4.67.1)\n",
            "Requirement already satisfied: datasets>=2.14.6 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from dspy) (3.3.2)\n",
            "Requirement already satisfied: optuna>=3.4.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from dspy) (4.2.1)\n",
            "Requirement already satisfied: magicattr>=0.1.6 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from dspy) (0.1.6)\n",
            "Requirement already satisfied: litellm>=1.60.3 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from dspy) (1.63.7)\n",
            "Requirement already satisfied: diskcache>=5.6.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from dspy) (5.6.3)\n",
            "Requirement already satisfied: json-repair>=0.30.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from dspy) (0.40.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from dspy) (9.0.0)\n",
            "Requirement already satisfied: anyio in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from dspy) (4.8.0)\n",
            "Requirement already satisfied: asyncer==0.0.8 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from dspy) (0.0.8)\n",
            "Requirement already satisfied: cachetools>=5.5.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from dspy) (5.5.2)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from dspy) (3.1.1)\n",
            "Requirement already satisfied: rich>=13.7.1 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from dspy) (13.9.4)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from dspy) (2.1.3)\n",
            "Requirement already satisfied: idna>=2.8 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from anyio->dspy) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from anyio->dspy) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from botocore<1.38.0,>=1.37.24->boto3) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from botocore<1.38.0,>=1.37.24->boto3) (2.3.0)\n",
            "Requirement already satisfied: filelock in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from datasets>=2.14.6->dspy) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from datasets>=2.14.6->dspy) (19.0.1)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from datasets>=2.14.6->dspy) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from datasets>=2.14.6->dspy) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from datasets>=2.14.6->dspy) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.14.6->dspy) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from datasets>=2.14.6->dspy) (3.11.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from datasets>=2.14.6->dspy) (0.29.1)\n",
            "Requirement already satisfied: packaging in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from datasets>=2.14.6->dspy) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from datasets>=2.14.6->dspy) (6.0.2)\n",
            "Requirement already satisfied: certifi in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from httpx>=0.21.2->cohere) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from httpx>=0.21.2->cohere) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.21.2->cohere) (0.14.0)\n",
            "Requirement already satisfied: click in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from litellm>=1.60.3->dspy) (8.1.8)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from litellm>=1.60.3->dspy) (8.6.1)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from litellm>=1.60.3->dspy) (3.1.5)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from litellm>=1.60.3->dspy) (4.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.2.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from litellm>=1.60.3->dspy) (1.1.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from litellm>=1.60.3->dspy) (0.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from openai<=1.61.0,>=0.28.1->dspy) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from openai<=1.61.0,>=0.28.1->dspy) (0.8.2)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from optuna>=3.4.0->dspy) (1.15.2)\n",
            "Requirement already satisfied: colorlog in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from optuna>=3.4.0->dspy) (6.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from optuna>=3.4.0->dspy) (2.0.38)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from pandas>=2.1.1->dspy) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from pandas>=2.1.1->dspy) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from pydantic>=1.9.2->cohere) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.0.0->cohere) (3.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from rich>=13.7.1->dspy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from rich>=13.7.1->dspy) (2.19.1)\n",
            "Requirement already satisfied: Mako in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from alembic>=1.5.0->optuna>=3.4.0->dspy) (1.3.9)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.14.6->dspy) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.14.6->dspy) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.14.6->dspy) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.14.6->dspy) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.14.6->dspy) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.14.6->dspy) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from aiohttp->datasets>=2.14.6->dspy) (1.18.3)\n",
            "Requirement already satisfied: zipp>=3.20 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from importlib-metadata>=6.8.0->litellm>=1.60.3->dspy) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from jinja2<4.0.0,>=3.1.2->litellm>=1.60.3->dspy) (2.1.5)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.60.3->dspy) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.60.3->dspy) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.60.3->dspy) (0.22.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->dspy) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.38.0,>=1.37.24->boto3) (1.17.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from sqlalchemy>=1.4.2->optuna>=3.4.0->dspy) (3.1.1)\n"
          ]
        }
      ],
      "source": [
        "! pip install cohere boto3 dspy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sP3DkCHC_66L"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-01 11:17:53.707355: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1743499073.732584   58659 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1743499073.739639   58659 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1743499073.763928   58659 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1743499073.763961   58659 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1743499073.763963   58659 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1743499073.763965   58659 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-04-01 11:17:53.773122: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "from typing import Any, Optional\n",
        "\n",
        "import boto3\n",
        "import numpy as np\n",
        "import dspy\n",
        "from dspy.dsp.utils import dotdict\n",
        "from dspy import (\n",
        "    Example,\n",
        "    InputField,\n",
        "    Module,\n",
        "    OutputField,\n",
        "    Predict,\n",
        "    Prediction,\n",
        "    Retrieve,\n",
        "    Signature,\n",
        "    context,\n",
        ")\n",
        "from dspy.datasets import DataLoader\n",
        "from dspy.evaluate import Evaluate\n",
        "from sentence_transformers import SentenceTransformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_IM4l2f4_66N"
      },
      "outputs": [],
      "source": [
        "os.environ[\"COHERE_API_KEY\"] = \"EhIqPfTHeMWbPA4XiaYYZLuEoX9HyjrEHAh9sgaL\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shnOsZyk_66O"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7N-th00N_66P"
      },
      "source": [
        "We are going to work with the [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) dataset. To limit the use of resources, only a subset of the 10 questions will be used.\n",
        "\n",
        "For a corpus of context paragraphs from the first 100 questions in the validation split will be used.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9GAd6y-L_66P"
      },
      "outputs": [],
      "source": [
        "data = DataLoader().from_huggingface(\n",
        "    dataset_name=\"rajpurkar/squad\",\n",
        ")\n",
        "validation = data[\"validation\"][:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yRmSPcF6_66Q"
      },
      "outputs": [],
      "source": [
        "corpus = [example.context for example in data[\"validation\"][:100]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmqXCicH_66R"
      },
      "source": [
        "We are going to work in a zero-shot scenario i.e. LLMs will not be trained to answer our questions. The LLM used to answer the question is Command-r-plus from Cohere."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "I-oI8gNF_66i"
      },
      "outputs": [],
      "source": [
        "lm = dspy.LM('cohere/command-r-plus', api_key=os.environ[\"COHERE_API_KEY\"])\n",
        "dspy.configure(lm=lm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QX3p8V5_66j"
      },
      "source": [
        "# Simple Question Answering pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-geut19q_66j"
      },
      "source": [
        "For reference you are given the implementation of the vanilla question answering pipeline. It will be used as a baseline.\n",
        "The pipeline asks the question to LLM and returns model's answer. The pipeline can be created as a dspy [Signature](https://dspy-docs.vercel.app/docs/deep-dive/signature/understanding-signatures)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uKRhyw8C_66j"
      },
      "outputs": [],
      "source": [
        "class VanillaQuestionAnswering(Signature):\n",
        "    \"\"\"Answer questions with short factually correct answers.\"\"\"\n",
        "\n",
        "    question = InputField()\n",
        "    answer = OutputField(desc=\"Answer is often short and educational.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Ey4PwOVj_66k"
      },
      "outputs": [],
      "source": [
        "with context(lm=lm):\n",
        "    qa_pipeline = Predict(VanillaQuestionAnswering)\n",
        "    prediction = qa_pipeline(\n",
        "        question=\"Which NFL team represented the AFC at Super Bowl 50?\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KcZVmqS2_66l"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('The Denver Broncos represented the AFC at Super Bowl 50.',\n",
              " 'Correct answer: Denver Broncos')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prediction.answer, f\"Correct answer: Denver Broncos\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFA71lOr_66l"
      },
      "source": [
        "# Retrieval Augmented Generation (RAG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vyu-MA2Y_66l"
      },
      "source": [
        "Your will implement the Retrieval Augmented Generatio pipeline. You can read more about RAG [here](https://blogs.nvidia.com/blog/what-is-retrieval-augmented-generation/).\n",
        "You tasks are:\n",
        "* Implement a custom [Retriever](https://dspy-docs.vercel.app/docs/deep-dive/retrieval_models_clients/custom-rm-client) (follow the interface below).\n",
        "* Implement a a custom [dspy.Module](https://dspy-docs.vercel.app/docs/deep-dive/modules/guide) (follow the interface below)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "rTJBTcWv_66l"
      },
      "outputs": [],
      "source": [
        "class VanillaRetriever(Retrieve):\n",
        "    def __init__(self, corpus: list[str], k: int = 3):\n",
        "        ############### TODO ###############\n",
        "        self.embedder = dspy.Embedder(\n",
        "            SentenceTransformer('all-MiniLM-L6-v2').encode,\n",
        "            batch_size=100,\n",
        "        )\n",
        "\n",
        "        self.k = k\n",
        "\n",
        "        self.embeddings = self._normalize_embeddings(self.embedder(corpus))\n",
        "\n",
        "        self.corpus = np.array(corpus)\n",
        "        ####################################\n",
        "\n",
        "    def _normalize_embeddings(self, embeddings: np.ndarray) -> np.ndarray:\n",
        "        ############### TODO ###############\n",
        "        l = embeddings.shape[0]\n",
        "        lengths = np.sum(embeddings * embeddings, axis = 1).reshape(l, 1)\n",
        "        return embeddings / lengths \n",
        "        ####################################\n",
        "\n",
        "    def forward(self, query_or_queries: str, k: Optional[int] = None) -> Prediction:\n",
        "        ############### TODO ###############\n",
        "        if k is None:\n",
        "            k = self.k \n",
        "\n",
        "        query_or_queries = [query_or_queries]\n",
        "\n",
        "        query_embedding = self._normalize_embeddings(\n",
        "            self.embedder(query_or_queries)\n",
        "        )\n",
        "\n",
        "        queries = len(query_or_queries)\n",
        "        embeddings = np.tile(\n",
        "            self.embeddings.reshape(100, 1, -1),\n",
        "            (1, queries, 1),\n",
        "        )\n",
        "\n",
        "        dot_products = np.sum(embeddings * query_embedding, axis = -1)\n",
        "\n",
        "        indices = np.argpartition(dot_products, -(k + 1), axis = 0)[-(k + 1):]\n",
        "        indices = indices.ravel().astype(int).tolist()\n",
        "\n",
        "        # result = self.corpus[indices]\n",
        "\n",
        "        Prediction(\n",
        "            passages=[dotdict({\"long_text\": self.corpus[idx]}) for idx in indices]\n",
        "        )\n",
        "        ####################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0RaFCFP_66m"
      },
      "outputs": [],
      "source": [
        "retrieval = VanillaRetriever(corpus=corpus, k=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "JfpnuC3r_66m"
      },
      "outputs": [],
      "source": [
        "class RAGSignature(Signature):\n",
        "    \"\"\"Answer questions with short answers. Output only answer.\"\"\"\n",
        "\n",
        "    context = InputField(desc=\"may contain relevant information.\")\n",
        "    question = InputField(desc=\"User question\")\n",
        "    answer = OutputField(desc=\"answer is often concise and educational.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLpzt32S_66m"
      },
      "outputs": [],
      "source": [
        "class RAG(Module):\n",
        "    def __init__(self, corpus: list[str], num_passages: int) -> None:\n",
        "        super().__init__()\n",
        "        ############### TODO ###############\n",
        "        self.retriever = VanillaRetriever(corpus, k = 3)\n",
        "        self.answer = Predict(RAGSignature)\n",
        "        ####################################\n",
        "\n",
        "    def forward(self, question: str, **kwargs: Any) -> Predict:\n",
        "        ############### TODO ###############\n",
        "        context = self.retriever(question).passages\n",
        "        pred = self.answer(context = context, question = question)\n",
        "\n",
        "        return Prediction(context = context, answer = pred.answer)\n",
        "        ####################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "XKxIpy2h_66n"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'passages'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[52], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context(lm\u001b[38;5;241m=\u001b[39mlm):\n\u001b[1;32m      2\u001b[0m     rag_pipeline \u001b[38;5;241m=\u001b[39m RAG(corpus\u001b[38;5;241m=\u001b[39mcorpus, num_passages\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mrag_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhich NFL team represented the AFC at Super Bowl 50?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Git/ML-stuff/venv/lib/python3.12/site-packages/dspy/utils/callback.py:266\u001b[0m, in \u001b[0;36mwith_callbacks.<locals>.wrapper\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# If no callbacks are provided, just call the function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callbacks:\n\u001b[0;32m--> 266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# Generate call ID as the unique identifier for the call, this is useful for instrumentation.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m call_id \u001b[38;5;241m=\u001b[39m uuid\u001b[38;5;241m.\u001b[39muuid4()\u001b[38;5;241m.\u001b[39mhex\n",
            "File \u001b[0;32m~/Git/ML-stuff/venv/lib/python3.12/site-packages/dspy/primitives/program.py:30\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m         output\u001b[38;5;241m.\u001b[39mset_lm_usage(usage_tracker\u001b[38;5;241m.\u001b[39mget_total_tokens())\n\u001b[1;32m     28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[51], line 11\u001b[0m, in \u001b[0;36mRAG.forward\u001b[0;34m(self, question, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, question: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Predict:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m############### TODO ###############\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpassages\u001b[49m\n\u001b[1;32m     12\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manswer(context \u001b[38;5;241m=\u001b[39m context, question \u001b[38;5;241m=\u001b[39m question)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Prediction(context \u001b[38;5;241m=\u001b[39m context, answer \u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39manswer)\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'passages'"
          ]
        }
      ],
      "source": [
        "with context(lm=lm):\n",
        "    rag_pipeline = RAG(corpus=corpus, num_passages=2)\n",
        "    prediction = rag_pipeline(\n",
        "        question=\"Which NFL team represented the AFC at Super Bowl 50?\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9taOiyF_66n"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('The Denver Broncos represented the AFC at Super Bowl 50.',\n",
              " 'Correct answer: Denver Broncos')"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prediction.answer, f\"Correct answer: Denver Broncos\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3IKisHu_66n"
      },
      "source": [
        "# LLM as a judge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MUWQHYU_66n"
      },
      "source": [
        "Evaluation of tasks based on text generation such as Question Answering poses a challenge. One can:\n",
        "* Evaluate manually\n",
        "* Develop automated metrics\n",
        "\n",
        "Recently, the third option emerged:\n",
        "* Ask the LLM to do the evaluation for us!\n",
        "\n",
        "In this section you will implement the LLM as a jude evaluatio of vanilla QA and RAG pipelines. Your tasks are:\n",
        "* Implement the factuality_metric, which asks the LLM to assess if the candidate answer is factually correct (follow the interface below)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-RNZixS_66n"
      },
      "outputs": [],
      "source": [
        "class LLMJudge(Signature):\n",
        "    \"\"\"Assess the quality of the answer along the specified criterion.\"\"\"\n",
        "\n",
        "    answer = InputField(desc=\"Candidate answer for the question\")\n",
        "    question = InputField(desc=\"Question to be answered\")\n",
        "    golden_answer = InputField(desc=\"The golden correct answer for the question\")\n",
        "    criterion = InputField(desc=\"criterion:\")\n",
        "    judgement = OutputField(\n",
        "        desc=\"Answer Yes or No based on the criterion\",\n",
        "        prefix=\"Yes or No\",\n",
        "    )\n",
        "\n",
        "\n",
        "judge = Predict(LLMJudge)\n",
        "\n",
        "\n",
        "def factuality_metric(example: Example, pred: Prediction) -> int:\n",
        "    ############### TODO ###############\n",
        "    ####################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaWub56M_66o"
      },
      "outputs": [],
      "source": [
        "devset = [\n",
        "    Example(question=e.question, golden_answer=e.answers[\"text\"][0]).with_inputs(\n",
        "        \"question\", \"golden_answer\"\n",
        "    )\n",
        "    for e in validation\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eifDVf8t_66o"
      },
      "outputs": [],
      "source": [
        "with context(lm=lm):\n",
        "    evaluate_pipeline = Evaluate(\n",
        "        devset=devset,\n",
        "        metric=factuality_metric,\n",
        "        num_threads=1,\n",
        "        display_progress=True,\n",
        "        display_table=100,\n",
        "    )\n",
        "    results_rag = evaluate_pipeline(rag_pipeline)\n",
        "    results_qa = evaluate_pipeline(qa_pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeLJOCuQ_66o"
      },
      "outputs": [],
      "source": [
        "results_qa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dV_Xsepv_66o"
      },
      "outputs": [],
      "source": [
        "results_rag"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
