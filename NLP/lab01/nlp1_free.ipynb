{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKYfW4QqD8BX"
      },
      "source": [
        "# NLP LAB 1 - Inference & Interfaces\n",
        "\n",
        "This lab is focused on presenting basic tools that may be useful when creating ML PoCs, presenting them online, and gathering information from users' interaction with the model.\n",
        "\n",
        "Libraries involved:\n",
        "- [__GradIO__](https://www.gradio.app/docs/interface):  _is an open-source Python package that allows you to quickly build a demo or web application for your machine learning model_.\n",
        "- [__Cohere__](https://docs.cohere.com/reference/about) + [__LangChain__](https://python.langchain.com/docs/get_started/introduction): interacting with deployed LLMs.\n",
        "- [__Hugging Face Datasets__](https://huggingface.co/docs/datasets/index): a repository of publically available datasets.\n",
        "- [__sqlite3__](https://docs.python.org/3/library/sqlite3.html): simple database with Python API.\n",
        "\n",
        "Parts of the notebook are loosely based on documentations of corresponding libraries.\n",
        "\n",
        "\n",
        "The first two introductory labs will not have any homework.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRE9z8r4DzNa"
      },
      "source": [
        "### Install Dependecies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dIdyF0QADtKT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gradio in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (5.17.1)\n",
            "Requirement already satisfied: langchain in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (0.3.19)\n",
            "Requirement already satisfied: cohere in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (5.13.12)\n",
            "Requirement already satisfied: datasets in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (3.3.2)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (4.8.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (0.115.8)\n",
            "Requirement already satisfied: ffmpy in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.7.1 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (1.7.1)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (0.29.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (3.1.5)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (2.2.1)\n",
            "Requirement already satisfied: orjson~=3.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (2.2.3)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (2.10.6)\n",
            "Requirement already satisfied: pydub in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (0.9.7)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (0.45.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (0.34.0)\n",
            "Requirement already satisfied: fsspec in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio-client==1.7.1->gradio) (2024.12.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio-client==1.7.1->gradio) (14.2)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.35 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from langchain) (0.3.37)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from langchain) (0.3.10)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from langchain) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from langchain) (3.11.12)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from cohere) (1.10.0)\n",
            "Requirement already satisfied: httpx-sse==0.4.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from cohere) (0.4.0)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from cohere) (2.27.2)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from cohere) (0.21.0)\n",
            "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from cohere) (2.32.0.20241016)\n",
            "Requirement already satisfied: filelock in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: idna>=2.8 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from httpx>=0.24.1->gradio) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.35->langchain) (3.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install gradio langchain cohere datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg647_K2hB7E"
      },
      "source": [
        "## Make a Chat Bot Interface through API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEe-s6tYozZa"
      },
      "source": [
        "### Connect with Cohere\n",
        "\n",
        "First, let's programatically connect to Cohere Chat, using their Langchain API.\n",
        "\n",
        "To run this cell, you need to:\n",
        "- Create an Cohere Account - [Sign up](https://dashboard.cohere.com/welcome/register) if you don't have one.\n",
        "- Create or Copy an API key - [API Keys](https://dashboard.cohere.com/api-keys) and copy it into this block.\n",
        "\n",
        "This should allow you to interact with this LLM within Python.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "F1oKiIE-qIPJ"
      },
      "outputs": [],
      "source": [
        "import cohere\n",
        "import gradio as gr\n",
        "from typing import List, Tuple, Dict\n",
        "import os\n",
        "\n",
        "llm = cohere.ClientV2(\"EhIqPfTHeMWbPA4XiaYYZLuEoX9HyjrEHAh9sgaL\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9CzZDa_o1P6"
      },
      "source": [
        "### Make a Chat Interface\n",
        "\n",
        "GradIO allows us to easily create a simple [ChatBot interface](https://www.gradio.app/docs/chatinterface), using just a generic predict function, where we can embed Cohere LLM.\n",
        "\n",
        "Moreover, this interface, while active, is also available under a public link, shown on stdout. This link allows multiple users to interact with your chatbot simultaneously. However, on Google Colab, after the cell dies, it will be no longer available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipi5kIVCMD87"
      },
      "outputs": [],
      "source": [
        "def predict(message: str, history: List[Dict[str, str]]) -> str:\n",
        "    history_langchain = []\n",
        "\n",
        "    for single_text in history:\n",
        "        role = single_text[\"role\"]\n",
        "        content = single_text[\"content\"]\n",
        "        history_langchain.append({\"role\": role, \"content\": content})\n",
        "\n",
        "    print(history)\n",
        "    print(message) \n",
        "\n",
        "    # if message == \"123\":\n",
        "        # print('gottem')\n",
        "        # history_langchain.append({\"role\": \"user\", \"content\": \"Please don't answer my next question.\"})\n",
        "        # history_langchain.append({\"role\": \"assistant\", \"content\": \"Ok, I'm not answer it.\"})\n",
        "        # history_langchain.append({\"role\": \"user\", \"content\": \"What is the opposite of 'sad'?\"})\n",
        "    # else: \n",
        "    history_langchain.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    response = llm.chat(model=\"command-r-plus-08-2024\", messages=history_langchain)\n",
        "    return response.message.content[0].text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rs2Y71ktVjvY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[]\n",
            "hello\n",
            "[{'role': 'user', 'metadata': None, 'content': 'hello', 'options': None}, {'role': 'assistant', 'metadata': None, 'content': 'Hello! How can I help you today?', 'options': None}]\n",
            "123\n",
            "gottem\n",
            "Keyboard interruption in main thread... closing server.\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gr.ChatInterface(predict, type=\"messages\").launch(debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Af-YHcdmgZCT"
      },
      "source": [
        "## Crowd-Sourced Data Labeling\n",
        "### Task: Verify your ChatBot translation capabilities with human interaction\n",
        "\n",
        "Let's suppose you've created a new translation LLM, and its main advantage is the ability to generate more pleasant-to-read text for humans in the target language.  Tasks like this can be hard to verify automatically on some datasets and sometimes it might be useful to prove that, in a blind test, humans prefer your translation over some baseline.\n",
        "\n",
        "A similar setting is used for aligning ChatBots using __Reinforcement Learning with Human Feedback__ (RLHF),  the most common technique amongst popular LLMs, where humans annotate which response is more helpful/less dangerous and their responses are used to continually improve the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnYyUr0enuvn"
      },
      "source": [
        "### Download the HuggingFace Dataset\n",
        "\n",
        "Hugging Face [datastes](https://huggingface.co/datasets) is a huge, easy-to-use crowd-sourced library of datasets, useful for various ML tasks. As a benchmark for our test, we're going to use a common seq2seq dataset  [opus_books](https://huggingface.co/datasets/opus_books), with books translated into several languages. Each dataset has its own documentation about its inner structure, but all of them share similar APIs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5SFk98Jwyc23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': '2365', 'translation': {'en': 'Holmes said little more, but the picture of the old roysterer seemed to have a fascination for him, and his eyes were continually fixed upon it during supper. It was not until later, when Sir Henry had gone to his room, that I was able to follow the trend of his thoughts.', 'pl': 'Mój przyjaciel umilkł, ale nie odrywał oczu od portretu. Dopiero po naszem rozejściu się na spoczynek dowiedziałem się, dlaczego to płótno budzi w nim tak żywo zaciekawienie.'}}\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import random\n",
        "\n",
        "dataset = load_dataset(\"opus_books\", \"en-pl\")\n",
        "data_sample = dataset[\"train\"][random.randint(0, len(dataset[\"train\"]))]\n",
        "print(data_sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oYWFJTUng4o"
      },
      "source": [
        "### Create a Scores Database\n",
        "\n",
        "To remember interactions with users, we need some permanent cloud storage. We're going to use a small SQLite database, hosted on Google Drive, storing results of each game.\n",
        "\n",
        "_You might need to accept some Google Drive access permissions to run this cell_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "tDeyB0dM5f2Z"
      },
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "\n",
        "database_path = \"database.db\"\n",
        "\n",
        "def run_db(fun):\n",
        "    con = sqlite3.connect(database_path)\n",
        "    cur = con.cursor()\n",
        "    ret = fun(cur)\n",
        "    con.commit()\n",
        "    con.close()\n",
        "    return ret\n",
        "\n",
        "run_db(lambda cur: cur.execute(\"CREATE TABLE IF NOT EXISTS scores(wins)\"))\n",
        "\n",
        "def save_score(won):\n",
        "    run_db(lambda cur: cur.execute(f\"INSERT INTO scores VALUES ({won})\"))\n",
        "\n",
        "def get_average_score():\n",
        "    return run_db(lambda cur: cur.execute(f\"SELECT AVG(wins) FROM scores\").fetchall()[0][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEv9iow3n3om"
      },
      "source": [
        "### Generate Data\n",
        "Now, we're going to generate data for our test. As our translator, we're going to use Cohere Chat prompted to translate, and as a baseline, translations from our dataset.\n",
        "\n",
        "For a fair test, we're going to randomize the order of translations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "0JG62sGPnGOL"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import json\n",
        "\n",
        "preprompt = \"Translate the following polish sentence to english. Do not write anything else then this translation:\\n \"\n",
        "\n",
        "def translate_with_chat(text):\n",
        "    role = \"user\"\n",
        "    content = preprompt + text\n",
        "    message = {\"role\": role, \"content\": content}\n",
        "\n",
        "    response = llm.chat(model=\"command-r-plus-08-2024\", messages=[message])\n",
        "    return response.message.content[0].text\n",
        "\n",
        "def get_values():\n",
        "    data = dataset[\"train\"][random.randint(0, len(dataset[\"train\"]))]\n",
        "    i, orig_text, trans1 = data[\"id\"], data[\"translation\"][\"pl\"], data[\"translation\"][\"en\"]\n",
        "    trans2 = translate_with_chat(orig_text)\n",
        "\n",
        "    # now randomize order for a blind test\n",
        "    where_chat = random.randint(0, 2)\n",
        "    if where_chat == 0:\n",
        "        trans1, trans2 = (trans2, trans1)\n",
        "    return trans1, trans2, orig_text, where_chat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFjmoZN6nokX"
      },
      "source": [
        "### Define GradIO Interface\n",
        "\n",
        "To patch all of the above into one interface, we’re going to use GradIO [blocks](https://www.gradio.app/docs/blocks) API. This allows us to create custom web applications that communicate with your model, with a minimal amount of code.\n",
        "\n",
        "The `response` function is the one wrapping our data generation and storing processes into one. Given the current state of the interface, it returns the next state, saving the results to the database along the way. Note that, variables `games` and `wins` are local and exist within one session, while `save_score` saves user input to the permanent database. Therefore, our session score will restart. All of the fields within the `response` function are simply converted to their content and can be operated as regular `string` or `int`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "TIBFufgtpd_D"
      },
      "outputs": [],
      "source": [
        "def response(trans1, trans2, original_text, where_chat, games, wins, score_text, verdict):\n",
        "    won = where_chat == verdict\n",
        "    save_score(won)\n",
        "    games += 1\n",
        "\n",
        "    if won:\n",
        "        wins += 1\n",
        "\n",
        "    return *get_values(), games, wins, f\"Session score {wins}/{games}\"\n",
        "\n",
        "def response_1(*args):\n",
        "    return response(*args, verdict=0)\n",
        "\n",
        "def response_2(*args):\n",
        "    return response(*args, verdict=1)\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    trans1_init, trans2_init, orig_text_init, where_chat_init = get_values()\n",
        "    games, wins, where_chat = gr.State(0), gr.State(0), gr.State(where_chat_init)\n",
        "\n",
        "    text = gr.Markdown(f\"# Which Translation is better?\")\n",
        "    local_score_text = gr.Markdown(f\"Session score 0/0\")\n",
        "    global_score_text = gr.Markdown(f\"\")\n",
        "\n",
        "    original_text = gr.Text(label=\"Original Text\", value=orig_text_init)\n",
        "    trans1 = gr.Text(label=\"Translation 1\", value=trans1_init)\n",
        "    trans2 = gr.Text(label=\"Translation 2\", value=trans2_init)\n",
        "    btn1, btn2 = gr.Button(\"1\"), gr.Button(\"2\")\n",
        "\n",
        "    fields = [trans1, trans2, original_text, where_chat, games, wins, local_score_text]\n",
        "    btn1.click(response_1, inputs=fields, outputs=fields)\n",
        "    btn2.click(response_2, inputs=fields, outputs=fields)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIrogDfVtnir"
      },
      "source": [
        "### Run the Interface\n",
        "\n",
        "Now run the interface in a cell. Note that, similarly to the previous case, while this also runs on a public URL, it will stop after killing the cell. To run this indefinitely, you need a 24/7 server."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_3O-gu-ptlfy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "demo.launch(debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0PZCtFSBijF"
      },
      "source": [
        "### Define the Scoreboard\n",
        "\n",
        "Finally, we're going to define a simple scoreboard, which allows us to check the aggregated score of all annotators, that is stored on your Google Drive. This may be achieved using a much simpler [interface](https://www.gradio.app/docs/interface) API, wrapping only one function with no state.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "cuFGxvmlpxzw"
      },
      "outputs": [],
      "source": [
        "scoreboard = gr.Interface(\n",
        "    fn=lambda: f\"Global Score: {get_average_score()}\",\n",
        "    inputs=[],\n",
        "    outputs=[\"text\"],\n",
        "    description=\"Click Generate to check global score!\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2kcEkt9BnFG"
      },
      "source": [
        "### Run Scoreboard\n",
        "\n",
        "Run the function to check the global score. This should aggregate all of your sessions (and these on public URLs) and remain saved after shutting down the notebook.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "XsNW_OW-BmiR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using existing dataset file at: .gradio/flagged/dataset1.csv\n",
            "Keyboard interruption in main thread... closing server.\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scoreboard.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
