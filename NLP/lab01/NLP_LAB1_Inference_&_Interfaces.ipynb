{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKYfW4QqD8BX"
      },
      "source": [
        "# NLP LAB 1 - Inference & Interfaces\n",
        "\n",
        "This lab is focused on presenting basic tools that may be useful when creating ML PoCs, presenting them online, and gathering information from users' interaction with the model.\n",
        "\n",
        "Libraries involved:\n",
        "- [__GradIO__](https://www.gradio.app/docs/interface):  _is an open-source Python package that allows you to quickly build a demo or web application for your machine learning model_.\n",
        "- [__OpenAI__](https://platform.openai.com/docs/introduction) + [__LangChain__](https://python.langchain.com/docs/get_started/introduction): interacting with deployed LLMs.\n",
        "- [__Hugging Face Datasets__](https://huggingface.co/docs/datasets/index): a repository of publically available datasets.\n",
        "- [__sqlite3__](https://docs.python.org/3/library/sqlite3.html): simple database with Python API.\n",
        "\n",
        "Parts of the notebook are loosely based on documentations of corresponding libraries.\n",
        "\n",
        "\n",
        "The first two introductory labs will not have any homework.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRE9z8r4DzNa"
      },
      "source": [
        "### Install Dependecies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dIdyF0QADtKT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gradio in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (5.17.1)\n",
            "Requirement already satisfied: langchain in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (0.3.19)\n",
            "Requirement already satisfied: langchain-openai in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (0.3.6)\n",
            "Requirement already satisfied: openai in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (1.64.0)\n",
            "Requirement already satisfied: datasets in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (3.3.2)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (4.8.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (0.115.8)\n",
            "Requirement already satisfied: ffmpy in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.7.1 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (1.7.1)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (0.29.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (3.1.5)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (2.2.1)\n",
            "Requirement already satisfied: orjson~=3.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (2.2.3)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (2.10.6)\n",
            "Requirement already satisfied: pydub in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (0.9.7)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (0.45.3)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio) (0.34.0)\n",
            "Requirement already satisfied: fsspec in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio-client==1.7.1->gradio) (2024.12.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from gradio-client==1.7.1->gradio) (14.2)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.35 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from langchain) (0.3.37)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from langchain) (0.3.10)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from langchain) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from langchain) (3.11.12)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: filelock in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: idna>=2.8 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: certifi in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from httpx>=0.24.1->gradio) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: click>=8.0.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.35->langchain) (3.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install gradio langchain langchain-openai openai datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg647_K2hB7E"
      },
      "source": [
        "## Make a Chat Bot Interface through API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEe-s6tYozZa"
      },
      "source": [
        "### Connect with OpenAI ChatGPT\n",
        "\n",
        "First, let's programatically connect to OpenAI ChatGPT, using their Langchain API.\n",
        "\n",
        "To run this cell, you need to:\n",
        "- Create an OpenAI Account - [Sign up](https://platform.openai.com/signup) if you don't have one.\n",
        "- Create an API key - [API Keys](https://platform.openai.com/api-keys) and copy it into this block.\n",
        "\n",
        "This should allow you to interact with this LLM within Python.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1oKiIE-qIPJ"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import AIMessage, HumanMessage\n",
        "import openai\n",
        "import gradio as gr\n",
        "from typing import List, Tuple\n",
        "import os\n",
        "\n",
        "# insert your key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "\n",
        "llm = ChatOpenAI(temperature=1.0, model='gpt-4o-mini')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9CzZDa_o1P6"
      },
      "source": [
        "### Make a Chat Interface\n",
        "\n",
        "GradIO allows us to easily create a simple [ChatBot interface](https://www.gradio.app/docs/chatinterface), using just a generic predict function, where we can embed ChatGPT.\n",
        "\n",
        "Moreover, this interface, while active, is also available under a public link, shown on stdout. This link allows multiple users to interact with your chatbot simultaneously. However, on Google Colab, after the cell dies, it will be no longer available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipi5kIVCMD87"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages/gradio/components/chatbot.py:291: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_18738/1593946814.py:7: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  gpt_response = llm(history_langchain)\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages/gradio/queueing.py\", line 625, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages/gradio/blocks.py\", line 2096, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages/gradio/blocks.py\", line 1641, in call_function\n",
            "    prediction = await fn(*processed_input)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages/gradio/utils.py\", line 857, in async_wrapper\n",
            "    response = await f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages/gradio/chat_interface.py\", line 862, in _submit_fn\n",
            "    response = await anyio.to_thread.run_sync(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 2461, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py\", line 962, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_18738/1593946814.py\", line 7, in predict\n",
            "    gpt_response = llm(history_langchain)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py\", line 181, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 1091, in __call__\n",
            "    generation = self.generate(\n",
            "                 ^^^^^^^^^^^^^^\n",
            "  File \"/home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 690, in generate\n",
            "    self._generate_with_cache(\n",
            "  File \"/home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py\", line 925, in _generate_with_cache\n",
            "    result = self._generate(\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py\", line 783, in _generate\n",
            "    response = self.client.create(**payload)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages/openai/_utils/_utils.py\", line 279, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py\", line 879, in create\n",
            "    return self._post(\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1290, in post\n",
            "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages/openai/_base_client.py\", line 967, in request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1056, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1105, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1056, in _request\n",
            "    return self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1105, in _retry_request\n",
            "    return self._request(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/home/nojak/Git/ML-stuff/venv/lib/python3.12/site-packages/openai/_base_client.py\", line 1071, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
          ]
        }
      ],
      "source": [
        "def predict(message: str, history: List[Tuple[str, str]]) -> str:\n",
        "    history_langchain = []\n",
        "    for human, ai in history:\n",
        "        history_langchain.append(HumanMessage(content=human))\n",
        "        history_langchain.append(AIMessage(content=ai))\n",
        "    history_langchain.append(HumanMessage(content=message))\n",
        "    gpt_response = llm(history_langchain)\n",
        "    return gpt_response.content\n",
        "\n",
        "gr.ChatInterface(predict).launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Af-YHcdmgZCT"
      },
      "source": [
        "## Crowd-Sourced Data Labeling\n",
        "### Task: Verify your ChatBot translation capabilities with human interaction\n",
        "\n",
        "Let's suppose you've created a new translation LLM, and its main advantage is the ability to generate more pleasant-to-read text for humans in the target language.  Tasks like this can be hard to verify automatically on some datasets and sometimes it might be useful to prove that, in a blind test, humans prefer your translation over some baseline.\n",
        "\n",
        "A similar setting is used for aligning ChatBots using __Reinforcement Learning with Human Feedback__ (RLHF),  the most common technique amongst popular LLMs, where humans annotate which response is more helpful/less dangerous and their responses are used to continually improve the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnYyUr0enuvn"
      },
      "source": [
        "### Download the HuggingFace Dataset\n",
        "\n",
        "Hugging Face [datastes](https://huggingface.co/datasets) is a huge, easy-to-use crowd-sourced library of datasets, useful for various ML tasks. As a benchmark for our test, we're going to use a common seq2seq dataset  [opus_books](https://huggingface.co/datasets/opus_books), with books translated into several languages. Each dataset has its own documentation about its inner structure, but all of them share similar APIs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SFk98Jwyc23"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import random\n",
        "\n",
        "dataset = load_dataset(\"opus_books\", \"en-pl\")\n",
        "data_sample = dataset[\"train\"][random.randint(0, len(dataset[\"train\"]))]\n",
        "print(data_sample)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oYWFJTUng4o"
      },
      "source": [
        "###Create a Scores Database\n",
        "\n",
        "To remember interactions with users, we need some permanent cloud storage. We're going to use a small SQLite database, hosted on Google Drive, storing results of each game.\n",
        "\n",
        "_You might need to accept some Google Drive access permissions to run this cell_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tDeyB0dM5f2Z"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "import sqlite3\n",
        "\n",
        "# drive.mount('/content/gdrive')\n",
        "database_path = \"database.db\"\n",
        "\n",
        "def run_db(fun):\n",
        "  con = sqlite3.connect(database_path)\n",
        "  cur = con.cursor()\n",
        "  ret = fun(cur)\n",
        "  con.commit()\n",
        "  con.close()\n",
        "  return ret\n",
        "\n",
        "run_db(lambda cur: cur.execute(\"CREATE TABLE IF NOT EXISTS scores(wins)\"))\n",
        "\n",
        "def save_score(won):\n",
        "  run_db(lambda cur: cur.execute(f\"INSERT INTO scores VALUES ({won})\"))\n",
        "\n",
        "def get_average_score():\n",
        "  return run_db(lambda cur: cur.execute(f\"SELECT AVG(wins) FROM scores\").fetchall()[0][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEv9iow3n3om"
      },
      "source": [
        "### Generate Data\n",
        "Now, we're going to generate data for our test. As our translator, we're going to use Chat GPT prompted to translate, and as a baseline, translations from our dataset.\n",
        "\n",
        "For a fair test, we're going to randomize the order of translations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0JG62sGPnGOL"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import json\n",
        "\n",
        "preprompt = \"Translate the following polish sentence to english. Do not write anything else then this translation:\\n \"\n",
        "\n",
        "def translate_with_chatgpt(text):\n",
        "  gpt_response = llm([HumanMessage(preprompt + text)])\n",
        "  return  gpt_response.content\n",
        "\n",
        "def get_values():\n",
        "  data = dataset[\"train\"][random.randint(0, len(dataset[\"train\"]))]\n",
        "  i, orig_text, trans1 = data[\"id\"], data[\"translation\"][\"pl\"], data[\"translation\"][\"en\"]\n",
        "  trans2 = translate_with_chatgpt(orig_text)\n",
        "\n",
        "  # now randomize order for a blind test\n",
        "  where_gpt = random.randint(0, 2)\n",
        "  if where_gpt == 0:\n",
        "    trans1, trans2 = (trans2, trans1)\n",
        "  return trans1, trans2, orig_text, where_gpt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFjmoZN6nokX"
      },
      "source": [
        "### Define GradIO Interface\n",
        "\n",
        "To patch all of the above into one interface, weâ€™re going to use GradIO [blocks](https://www.gradio.app/docs/blocks) API. This allows us to create custom web applications that communicate with your model, with a minimal amount of code.\n",
        "\n",
        "The `response` function is the one wrapping our data generation and storing processes into one. Given the current state of the interface, it returns the next state, saving the results to the database along the way. Note that, variables `games` and `wins` are local and exist within one session, while `save_score` saves user input to the permanent database. Therefore, our session score will restart. All of the fields within the `response` function are simply converted to their content and can be operated as regular `string` or `int`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIBFufgtpd_D"
      },
      "outputs": [],
      "source": [
        "def response(trans1, trans2, original_text, where_gpt, games, wins, score_text, verdict):\n",
        "  won = where_gpt == verdict\n",
        "  save_score(won)\n",
        "  games += 1\n",
        "  if won: wins += 1\n",
        "  return *get_values(), games, wins, f\"Session score {wins}/{games}\"\n",
        "\n",
        "def response_1(*args): return response(*args, verdict=0)\n",
        "def response_2(*args): return response(*args, verdict=1)\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    trans1_init, trans2_init, orig_text_init, where_gpt_init = get_values()\n",
        "    games, wins, where_gpt = gr.State(0), gr.State(0), gr.State(where_gpt_init)\n",
        "\n",
        "    text = gr.Markdown(f\"# Which Translation is better?\")\n",
        "    local_score_text = gr.Markdown(f\"Session score 0/0\")\n",
        "    global_score_text = gr.Markdown(f\"\")\n",
        "\n",
        "    original_text = gr.Text(label=\"Original Text\", value=orig_text_init)\n",
        "    trans1 = gr.Text(label=\"Translation 1\", value=trans1_init)\n",
        "    trans2 = gr.Text(label=\"Translation 2\", value=trans2_init)\n",
        "    btn1, btn2 = gr.Button(\"1\"), gr.Button(\"2\")\n",
        "\n",
        "    fields = [trans1, trans2, original_text, where_gpt, games, wins, local_score_text]\n",
        "    btn1.click(response_1, inputs=fields, outputs=fields)\n",
        "    btn2.click(response_2, inputs=fields, outputs=fields)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIrogDfVtnir"
      },
      "source": [
        "### Run the Interface\n",
        "\n",
        "Now run the interface in a cell. Note that, similarly to the previous case, while this also runs on a public URL, it will stop after killing the cell. To run this indefinitely, you need a 24/7 server."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3O-gu-ptlfy"
      },
      "outputs": [],
      "source": [
        "demo.launch(debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0PZCtFSBijF"
      },
      "source": [
        "### Define the Scoreboard\n",
        "\n",
        "Finally, we're going to define a simple scoreboard, which allows us to check the aggregated score of all annotators, that is stored on your Google Drive. This may be achieved using a much simpler [interface](https://www.gradio.app/docs/interface) API, wrapping only one function with no state.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuFGxvmlpxzw"
      },
      "outputs": [],
      "source": [
        "scoreboard = gr.Interface(\n",
        "    fn=lambda: f\"Global Score: {get_average_score()}\",\n",
        "    inputs=[],\n",
        "    outputs=[\"text\"],\n",
        "    description=\"Click Generate to check global score!\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2kcEkt9BnFG"
      },
      "source": [
        "### Run Scoreboard\n",
        "\n",
        "Run the function to check the global score. This should aggregate all of your sessions (and these on public URLs) and remain saved after shutting down the notebook.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsNW_OW-BmiR"
      },
      "outputs": [],
      "source": [
        "scoreboard.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
