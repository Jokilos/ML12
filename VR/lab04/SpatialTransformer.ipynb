{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukBQefs3-hKI"
      },
      "source": [
        "# Spatial Transformer\n",
        "In this lab scenario, you will finish a implementation of a module that allows networks to perform spatial transformations on both input images and feature maps. For details, you can refer to [the paper](https://arxiv.org/abs/1506.02025).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsghGONx-hKJ"
      },
      "source": [
        "## Data Preparation\n",
        "For training, we are going to use the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightning"
      ],
      "metadata": {
        "id": "U6b82nZc-zhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lp8P7Bps-hKJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjpZ_1-q-hKK"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "NUM_CLASSES = 10\n",
        "IMAGE_HEIGHT = 28\n",
        "IMAGE_WIDTH = 28\n",
        "train_transforms = torchvision.transforms.Compose(\n",
        "    [\n",
        "        torchvision.transforms.ToTensor(),  # our input is an image\n",
        "        torchvision.transforms.RandomAffine(\n",
        "            45,\n",
        "            (0.25, 0.25),\n",
        "            scale=(0.5, 1.0),\n",
        "            interpolation=torchvision.transforms.InterpolationMode.BILINEAR,\n",
        "        ),\n",
        "        torchvision.transforms.Resize((IMAGE_HEIGHT, IMAGE_WIDTH)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "test_transforms = torchvision.transforms.Compose(\n",
        "    [\n",
        "        torchvision.transforms.ToTensor(),  # our input is an image\n",
        "        torchvision.transforms.Resize((IMAGE_HEIGHT, IMAGE_WIDTH)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "DOWNLOAD_PATH = \"~/torch_datasets/MNIST\"\n",
        "DATASET_TRAIN = torchvision.datasets.MNIST(\n",
        "    root=DOWNLOAD_PATH, train=True, transform=train_transforms, download=True\n",
        ")\n",
        "DATASET_TEST = torchvision.datasets.MNIST(\n",
        "    root=DOWNLOAD_PATH, train=False, transform=test_transforms, download=True\n",
        ")\n",
        "\n",
        "TRAIN_LOADER = torch.utils.data.DataLoader(\n",
        "    DATASET_TRAIN, batch_size=BATCH_SIZE, shuffle=True\n",
        ")\n",
        "TEST_LOADER = torch.utils.data.DataLoader(\n",
        "    DATASET_TEST, batch_size=BATCH_SIZE, shuffle=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IdjINYC-hKK"
      },
      "outputs": [],
      "source": [
        "def show_images(images, num_rows=1):\n",
        "    \"\"\"\n",
        "    Given a tensor of shape [BATCH, C, H, W]\n",
        "    prints BATCH images splitting them evenly among num_rows\n",
        "    \"\"\"\n",
        "    assert len(images.shape) == 4\n",
        "    num_images = images.shape[0]\n",
        "    row_len = num_images // num_rows\n",
        "    assert row_len * num_rows == num_images\n",
        "    _, axes = plt.subplots(num_rows, row_len, figsize=(12, 12))\n",
        "    images = images.permute(0, 2, 3, 1).detach().numpy()\n",
        "\n",
        "    def handle_img(img, axe):\n",
        "        axe.axis(\"off\")\n",
        "        img = np.clip(img, 0, 1)\n",
        "\n",
        "        axe.imshow(img)\n",
        "\n",
        "    if num_images == 1:\n",
        "        handle_img(images[0], axes)\n",
        "    else:\n",
        "        for i, img in enumerate(images):\n",
        "            if num_rows == 1:\n",
        "                handle_img(img, axes[i])\n",
        "            else:\n",
        "                r = i // row_len\n",
        "                c = i % row_len\n",
        "                handle_img(img, axes[r, c])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMmU6duV-hKL"
      },
      "outputs": [],
      "source": [
        "SAMPLE_TRAIN, _ = next(iter(TRAIN_LOADER))\n",
        "SAMPLE_TRAIN = SAMPLE_TRAIN[:16]\n",
        "show_images(SAMPLE_TRAIN, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dSGhMv8-hKL"
      },
      "outputs": [],
      "source": [
        "SAMPLE_IMGS, _ = next(iter(TEST_LOADER))\n",
        "SAMPLE_IMGS = SAMPLE_IMGS[:16]\n",
        "show_images(SAMPLE_IMGS, 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tskvoFf3-hKL"
      },
      "source": [
        "## Spatial Tools\n",
        "Our goal is to train a model on the MNIST dataset that would be invariant to transformations like scale change and shifts.  \n",
        "To achieve this we are going to use the spatial transformer.  \n",
        "\n",
        "Briefly speaking, given a source image $I$ (or feature map) we want to produce a transformed image $I'$ (rotated, flipped, cropped, etc.) for further processing (for example by CNN). What is more, we want transformation parameters to depend on $I$ (we will produce them using a neural network that will input $I$).  \n",
        "To produce such a transformed image, for each pixel coordinates $x'$, $y'$ in  $I'$ we will produce pixel coordinates $x$, $y$ in $I$ \n",
        "such that the value of the pixel $x', y'$ in $I$ will be the value of the pixel $x, y$ in $I$.\n",
        "As our procedure will be able to produce coordinates with unknown values (between two pixels) we will use bilinear interpolation (in fact there is one more reason for this choice). \n",
        "For transforming $x', y'$ to $x, y$ we will restrict ourselves to the family of affine transformations.\n",
        "\n",
        "To be more precise let $x', y'$ be the coordinates of some pixel in the output image scaled so that \n",
        "all coordinates are between $-1$ and $1$ (inclusive).  \n",
        "To get the value of the pixel $x', y'$ in $I'$ we first calculate:\n",
        "\n",
        "$$\n",
        "\\left(\\begin{array}{c} \n",
        "x\\\\\n",
        "y\n",
        "\\end{array}\\right)\n",
        "=\n",
        "\\left(\\begin{array}{ccc} \n",
        "\\theta_1 & \\theta_2 & \\theta_3\\\\ \n",
        "\\theta_4 & \\theta_5 & \\theta_6\n",
        "\\end{array}\\right)\n",
        "\\left(\\begin{array}{c} \n",
        "x'\\\\ \n",
        "y'\\\\\n",
        "1\n",
        "\\end{array}\\right)\n",
        "$$ \n",
        "\n",
        "Then we treat $x, y$ as the coordinates of the pixel in $I$, treating coordinates in $\\{-1, 1\\}\\times\\{-1, 1\\}$ as corner pixels.  \n",
        "Finally, we set the  value of the pixel $x', y'$ in $I'$ to either:\n",
        "* $0$ if we landed outside of the image\n",
        "* the result of the bilinear interpolation between values of pixels in $I$  at coordinates\n",
        "    + $\\lfloor x \\rfloor, \\lfloor y \\rfloor$\n",
        "    + $\\lfloor x \\rfloor, \\lceil y \\rceil$\n",
        "    + $\\lceil x \\rceil, \\lfloor y \\rfloor$\n",
        "    + $\\lceil x \\rceil, \\lceil y \\rceil$\n",
        "\n",
        "\n",
        "If the image (or feature map) consists of many channels (colors), we apply the same transformation parameters to all channels.   \n",
        "\n",
        "\n",
        "Before you start the implementation one more question.  \n",
        "**Why do we use bilinear interpolation here instead of just picking the nearest pixel**?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUaiQc2t-hKM"
      },
      "source": [
        "Below your task is to finish the implementation of `SamplingGridGenerator` that for each pixel $p$ in the output image generates coordinates in the input image that will be the source of $p$ value.   \n",
        "Assume that each coordinate in the input image is in $[-1, 1]$.  \n",
        "Note that the produced coordinates can lie outside the image (we will take care of it later).  \n",
        "**Please don't use build-in functions like `affine_grid` or `grid_sample`**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wD9OXlnW-hKM"
      },
      "outputs": [],
      "source": [
        "class SamplingGridGenerator(torch.nn.Module):\n",
        "    def __init__(self, output_height, output_width):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_height = output_height\n",
        "        self.output_width = output_width\n",
        "\n",
        "    def forward(self, theta):\n",
        "        \"\"\"\n",
        "        Given parameters of the transformation theta of shape [BATCH, 2, 3]\n",
        "        returns a sampling_grid of shape [BATCH, output_height, output_width, 2]\n",
        "        such that at sampling_grid[b, x', y'] are the coordinates of the pixel\n",
        "        in the source image from which the value of pixel x', y' in\n",
        "        the transformed image will be sampled.\n",
        "        Assumes that each coordinate in the source image lies in the range [-1, 1].\n",
        "        Note that transformation can point to pixels outside of the source image\n",
        "        and this module does not clip values to lie inside the source image.\n",
        "        \"\"\"\n",
        "\n",
        "        assert len(theta.shape) == 3  # [BATCH, 2, 3]\n",
        "        assert theta.shape[1] == 2\n",
        "        assert theta.shape[2] == 3\n",
        "\n",
        "        # TODO {\n",
        "\n",
        "        # }\n",
        "\n",
        "        assert sampling_grid.shape == (\n",
        "            theta.shape[0],\n",
        "            self.output_height,\n",
        "            self.output_width,\n",
        "            2,\n",
        "        )  # [BATCH, H', W', 2]\n",
        "        return sampling_grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9Sj4Zha-hKM"
      },
      "outputs": [],
      "source": [
        "# Here we can test the implementation\n",
        "sgg = SamplingGridGenerator(4, 4)\n",
        "\n",
        "# t1 should give sth like\n",
        "# tensor([[-1.0000-1.0000j, -1.0000-0.3333j, -1.0000+0.3333j, -1.0000+1.0000j],\n",
        "#         [-0.3333-1.0000j, -0.3333-0.3333j, -0.3333+0.3333j, -0.3333+1.0000j],\n",
        "#         [ 0.3333-1.0000j,  0.3333-0.3333j,  0.3333+0.3333j,  0.3333+1.0000j],\n",
        "#         [ 1.0000-1.0000j,  1.0000-0.3333j,  1.0000+0.3333j,  1.0000+1.0000j]])\n",
        "t1 = sgg(torch.tensor([[[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]]]))[0]\n",
        "print(\"t1\")\n",
        "print(torch.view_as_complex(t1))\n",
        "\n",
        "t2 = sgg(torch.tensor([[[0.0, 1.0, 0.0], [1.0, 0.0, 0.0]]]))[0]\n",
        "print(\"t2 = t1 with x, y swapped\")\n",
        "print(torch.view_as_complex(t2))\n",
        "assert (t2[..., [1, 0]] == t1).all()\n",
        "\n",
        "\n",
        "t3 = sgg(torch.tensor([[[1.0, 0.0, 1.0], [0.0, 1.0, 1.0]]]))[0]\n",
        "print(\"t3 = t1 +1\")\n",
        "print(torch.view_as_complex(t3))\n",
        "assert (t3 == t1 + 1).all()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUXxLR-p-hKM"
      },
      "source": [
        "Now implement `GridSampler` that given the sampling grid created by `SamplingGridGenerator` and source image will create the transformed image.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAgcma_l-hKM"
      },
      "outputs": [],
      "source": [
        "class GridSampler(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Given the source_image of shape [B, C, H, W]\n",
        "    and sampling_grid of shape [B, H', W', 2]\n",
        "    generates the transformed_image of shape\n",
        "    [B, C, H', W'].\n",
        "    transformed_image[b, c, x', y'] is the value of pixel\n",
        "    at coordinates sampling_grid[b, x', y']\n",
        "    in the source_image[b, c].\n",
        "    Values {-1, 1}x{-1, 1} in sampling_grid correspond to corner pixels.\n",
        "    Pixels outside of the source_image are assumed to have\n",
        "    value 0.\n",
        "    Values of unknown pixels inside are obtained\n",
        "    using bilinear interpolation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, source_image, sampling_grid):\n",
        "        assert len(source_image.shape) == 4  # [B, C, H, W]\n",
        "        batch_size = source_image.shape[0]\n",
        "        input_height = source_image.shape[-2]\n",
        "        input_width = source_image.shape[-1]\n",
        "\n",
        "        # [B, H', W', 2]\n",
        "        assert len(sampling_grid.shape) == 4\n",
        "        assert sampling_grid.shape[0] == batch_size\n",
        "        assert sampling_grid.shape[-1] == 2\n",
        "\n",
        "        output_height = sampling_grid.shape[1]\n",
        "        output_width = sampling_grid.shape[2]\n",
        "\n",
        "        # TODO {\n",
        "\n",
        "        # }\n",
        "\n",
        "        assert transformed_image.shape[0] == source_image.shape[0]\n",
        "        assert transformed_image.shape[1] == source_image.shape[1]\n",
        "        assert transformed_image.shape[2] == output_height\n",
        "        assert transformed_image.shape[3] == output_width\n",
        "        return transformed_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDnrzp1S-hKN"
      },
      "source": [
        "Let's check whether created modules work as we expect."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nay6w6O9-hKN"
      },
      "outputs": [],
      "source": [
        "def visualize_transformation(transform_matrices, imgs):\n",
        "    sgg = SamplingGridGenerator(IMAGE_HEIGHT, IMAGE_WIDTH)\n",
        "    gs = GridSampler()\n",
        "    sampling_grid = sgg(transform_matrices)\n",
        "    res = gs(imgs, sampling_grid)\n",
        "    show_images(res, 4)\n",
        "\n",
        "\n",
        "identity = torch.tensor([[[1.0, 0.0, 0.0], [0.0, 1.0, 0.0]]] * SAMPLE_IMGS.shape[0])\n",
        "flip_dim1 = torch.tensor([[[-1.0, 0.0, 0.0], [0.0, 1.0, 0.0]]] * SAMPLE_IMGS.shape[0])\n",
        "flip_dim2 = torch.tensor([[[1.0, 0.0, 0.0], [0.0, -1.0, 0.0]]] * SAMPLE_IMGS.shape[0])\n",
        "rotate_90 = torch.tensor([[[0.0, -1.0, 0.0], [1.0, 0.0, 0.0]]] * SAMPLE_IMGS.shape[0])\n",
        "translate = torch.tensor([[[1.0, 0.0, 1.0], [0.0, 1.0, 0.5]]] * SAMPLE_IMGS.shape[0])\n",
        "other = torch.tensor([[[1.5, 0.0, 0.75], [0.0, 1.5, 0.75]]] * SAMPLE_IMGS.shape[0])\n",
        "\n",
        "\n",
        "visualize_transformation(identity, SAMPLE_IMGS)\n",
        "visualize_transformation(flip_dim1, SAMPLE_IMGS)\n",
        "visualize_transformation(flip_dim2, SAMPLE_IMGS)\n",
        "visualize_transformation(rotate_90, SAMPLE_IMGS)\n",
        "# test_transformation(translate, SAMPLE_IMGS)\n",
        "visualize_transformation(other, SAMPLE_IMGS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HapV3bOA-hKN"
      },
      "source": [
        "## Network\n",
        "\n",
        "Finish the implementation of `NetWithSpatialTransformer`.\n",
        "To get the parameters of the transformation use `LocNet` module that is already implemented."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ka1oDg0-hKN"
      },
      "outputs": [],
      "source": [
        "class LocNet(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Localisation network\n",
        "    given the image generates parameters for the transformation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels=1):\n",
        "        super().__init__()\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(in_channels=in_channels, out_channels=4, kernel_size=3),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3),\n",
        "            torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Flatten(),\n",
        "            torch.nn.Dropout(p=0.2),\n",
        "            torch.nn.Linear(200, 32),\n",
        "            torch.nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # we start with the identity\n",
        "        self.last_matrix = torch.nn.Parameter(torch.zeros((32, 6)))\n",
        "        self.last_bias = torch.nn.Parameter(\n",
        "            torch.tensor([1.0, 0.0, 0.0, 0.0, 1.0, 0.0])\n",
        "        )\n",
        "\n",
        "    def forward(self, images):\n",
        "        assert len(images.shape) == 4  # [B, C, H, W]\n",
        "        res = self.layers(images)\n",
        "        res = res @ self.last_matrix + self.last_bias\n",
        "        res = res.reshape(images.shape[0], 2, 3)\n",
        "        return res\n",
        "\n",
        "\n",
        "class NetWithSpatialTransformer(torch.nn.Module):\n",
        "    \"\"\" \n",
        "    Given the images first uses LocNet module to get\n",
        "    transformation parameters.\n",
        "    Then uses grid_generator to generate a sampling grid for each image \n",
        "    and grid_sampler to transform each image.\n",
        "    Then processes the transformed images with layers.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self, in_channels=1, image_height=IMAGE_HEIGHT, image_width=IMAGE_HEIGHT\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.loc_net = LocNet(in_channels=in_channels)\n",
        "        self.grid_generator = SamplingGridGenerator(\n",
        "            output_height=image_height, output_width=image_width\n",
        "        )\n",
        "        self.grid_sampler = GridSampler()\n",
        "        # MNIST is a very simple dataset. \n",
        "        # To strongly motivate spatial transformer we use a simple model there\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            torch.nn.Flatten(),\n",
        "            torch.nn.Dropout(),\n",
        "            torch.nn.Linear(784, 32),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(32, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, images):\n",
        "        assert len(images.shape) == 4  # [B, C, H, W]\n",
        "        # TODO {\n",
        "\n",
        "        # }\n",
        "        return self.layers(sampled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "562qC-wP-hKN"
      },
      "outputs": [],
      "source": [
        "net = NetWithSpatialTransformer()\n",
        "net(SAMPLE_IMGS).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPik-not-hKN"
      },
      "source": [
        "## Training\n",
        "Training loop is already implemented"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIzflyIL-hKN"
      },
      "outputs": [],
      "source": [
        "class PLSpatialTransformer(pl.LightningModule):\n",
        "    def __init__(self, model: torch.nn.Module):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = model\n",
        "        self.criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return TRAIN_LOADER\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return TEST_LOADER\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self.model(x)\n",
        "        loss = self.criterion(logits, y)\n",
        "        choosen = torch.argmax(logits, dim=-1)\n",
        "        acc = (choosen == y).type(torch.float32).mean()\n",
        "\n",
        "        # on-line metrics\n",
        "        self.log(\"train/loss\", loss.detach())\n",
        "        self.log(\"train/acc\", acc.detach())\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self.model(x)\n",
        "        choosen = torch.argmax(logits, dim=-1)\n",
        "        acc = (choosen == y).type(torch.float32).mean()\n",
        "\n",
        "        self.log(\"test/acc\", acc.detach(), on_epoch=True)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.model.parameters())\n",
        "        return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iR6UDWlj-hKN"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "!mkdir -p tb_logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3Hs46RG-hKO"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir tb_logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkQJ_256-hKO"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device(\"cuda\")\n",
        "\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "net = NetWithSpatialTransformer()\n",
        "net.to(DEVICE)\n",
        "\n",
        "plST = PLSpatialTransformer(model=net)\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    logger=TensorBoardLogger(\"tb_logs\", name=\"my_model\"),\n",
        "    accelerator=\"gpu\",\n",
        "    max_epochs=2,\n",
        "    check_val_every_n_epoch=1,\n",
        ")\n",
        "\n",
        "trainer.fit(plST)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdxtYPky-hKO"
      },
      "source": [
        "## Inspection\n",
        "Let's check what our spatial transformed learned.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geVJutcq-hKO"
      },
      "outputs": [],
      "source": [
        "net.to(DEVICE)\n",
        "## First lets check unaltered images\n",
        "imgs = SAMPLE_IMGS\n",
        "show_images(imgs, 4)\n",
        "visualize_transformation(net.loc_net(imgs.to(DEVICE)).cpu(), imgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8OZiHgA-hKO"
      },
      "outputs": [],
      "source": [
        "## Now lets check what spatial transformer will do with altered images from train set\n",
        "imgs = SAMPLE_TRAIN\n",
        "show_images(imgs, 4)\n",
        "visualize_transformation(net.loc_net(imgs.to(DEVICE)).cpu(), imgs)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}