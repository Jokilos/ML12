{"cells":[{"cell_type":"markdown","metadata":{"id":"HFbV9yzDQbRo"},"source":["# Intro\n","In this lab scenario you will implement  [Grad-CAM](https://arxiv.org/abs/1610.02391) and guided Grad-CAM.\n","Those two methods are used to explain what parts of the image are important for the model prediction.\n","\n","Grad-CAM produces a heatmap of important parts in the image.  \n","Guided Grad-CAM combines the output of Grad-Cam with gradients calculated with respect to input pixels."]},{"cell_type":"markdown","metadata":{"id":"-6r2gQulQbRq"},"source":["# Data and Visualizations"]},{"cell_type":"markdown","metadata":{"id":"E5EqBGyDQbRq"},"source":["## Dataset preparation\n","First, let's get the input images.\n","We will download a few zebra and a few magpie images from Wikipedia."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2TU1nMt0QbRq"},"outputs":[],"source":["from pathlib import Path\n","import os\n","\n","# we create directories for zebra and magpie images\n","!mkdir -p animals/zebra\n","!mkdir -p animals/magpie\n","\n","# we download zebra images\n","# By Ltshears at English Wikipedia - Transferred from en.wikipedia to Commons by Calliopejen1 using CommonsHelper., Public Domain, https://commons.wikimedia.org/w/index.php?curid=17695258\n","!wget https://upload.wikimedia.org/wikipedia/commons/7/77/Equus_zebra_hartmannae_%281%29.jpg?download -O animals/zebra/zebra1.jpg\n","#By Photographie de AndrĂŠ ALLIOT - Own work, CC0, https://commons.wikimedia.org/w/index.php?curid=95062050\n","!wget https://upload.wikimedia.org/wikipedia/commons/thumb/a/a7/Z%C3%A8bres_de_Gr%C3%A9vy.jpg/640px-Z%C3%A8bres_de_Gr%C3%A9vy.jpg?download -O animals/zebra/zebra2.jpg\n","#By George Brits georgebrits_cableandgrain - https://unsplash.com/photos/wvO5tPfTpugarchive copyImage, CC0, https://commons.wikimedia.org/w/index.php?curid=62235911\n","!wget https://upload.wikimedia.org/wikipedia/commons/thumb/b/b0/Zebra_squared_%28Unsplash%29.jpg/640px-Zebra_squared_%28Unsplash%29.jpg?download -O animals/zebra/zebra3.jpg\n","\n","# we download magpie images\n","# By Adrian Pingstone (Arpingstone) - Own work, Public Domain, https://commons.wikimedia.org/w/index.php?curid=3343840\n","!wget https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/Magpie_arp.jpg/373px-Magpie_arp.jpg?download -O animals/magpie/mp1.jpg\n","# By Vincent Oostelbos - https://www.inaturalist.org/photos/122369356, CC0, https://commons.wikimedia.org/w/index.php?curid=104787230\n","!wget https://upload.wikimedia.org/wikipedia/commons/thumb/4/44/Pica_pica_122369356.jpg/640px-Pica_pica_122369356.jpg?download -O animals/magpie/mp2.jpg\n","# By Аимаина хикари - Own work, CC0, https://commons.wikimedia.org/w/index.php?curid=27314113\n","!wget https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Pica_pica_fledgling2.JPG/556px-Pica_pica_fledgling2.JPG?download -O animals/magpie/mp3.jpg\n","\n","# directory with images\n","ROOT = Path(\"animals/\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jUhSE-OkPjdS","executionInfo":{"status":"ok","timestamp":1711532768741,"user_tz":-60,"elapsed":19950,"user":{"displayName":"Konrad Staniszewski","userId":"09334663474491114042"}},"outputId":"f14c7d8d-5997-4e08-9985-09a222831f14"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.25.2)\n"]}],"source":["!pip install opencv-python"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D-gSKsy0QbRr"},"outputs":[],"source":["import torch\n","import torchvision\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import cv2\n","from PIL import Image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qryfnng5QbRr"},"outputs":[],"source":["class ImagesDataset(torch.utils.data.Dataset):\n","    \"\"\"\n","    Loads images from the directory and applies transforms to them.\n","    \"\"\"\n","\n","    def __init__(self, directory: Path, transforms):\n","        self.transforms = transforms\n","        self.img_paths = sorted(list(directory.glob(\"*\")))\n","\n","    def __getitem__(self, idx):\n","        img = Image.open(self.img_paths[idx]).convert(\"RGB\")\n","\n","        img = self.transforms(img)\n","\n","        return img\n","\n","    def __len__(self):\n","        return len(self.img_paths)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6kM0nBdBQbRr"},"outputs":[],"source":["# as we are going to use a pre-trained model later\n","# we want to match its data distribution\n","image_net_mean = [0.485, 0.456, 0.406]\n","image_net_std = [0.229, 0.224, 0.225]\n","input_transforms = torchvision.transforms.Compose(\n","    [\n","        torchvision.transforms.Resize(\n","            (256, 256), interpolation=torchvision.transforms.InterpolationMode.BILINEAR\n","        ),\n","        torchvision.transforms.CenterCrop((224, 224)),\n","        torchvision.transforms.ToTensor(),  # our input is an image\n","        torchvision.transforms.Normalize(image_net_mean, image_net_std),\n","    ]\n",")\n","\n","\n","ZEBRA = ImagesDataset(ROOT / \"zebra\", input_transforms)\n","MAGPIE = ImagesDataset(ROOT / \"magpie\", input_transforms)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PQsQSaFDQbRr"},"outputs":[],"source":["SAMPLE_SIZE = 3\n","\n","LOADER_ZEBRA = torch.utils.data.DataLoader(\n","    ZEBRA, shuffle=False, batch_size=SAMPLE_SIZE\n",")\n","\n","LOADER_MAGPIE = torch.utils.data.DataLoader(\n","    MAGPIE, shuffle=False, batch_size=SAMPLE_SIZE\n",")\n","\n","\n","SAMPLE_ZEBRA = next(iter(LOADER_ZEBRA))\n","SAMPLE_MAGPIE = next(iter(LOADER_MAGPIE))"]},{"cell_type":"markdown","metadata":{"id":"ioPq1eKyQbRr"},"source":["## Visualizations\n","Below you can find a function that will help us inspect both images and Grad-CAM class-discriminative\n","localization map."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Wu0uQX1QbRr"},"outputs":[],"source":["def denormalize(imgs):\n","    return imgs * 0.225 + 0.5\n","\n","\n","def show_images(images):\n","    \"\"\"\n","    Given a tensor of shape [BATCH, C, H, W]\n","    prints BATCH images in one row\n","    for C = 1, those images are converted to heatmaps\n","    for C = 3, images are just printed\n","    for C = 4, the first channel is converted to a heatmap and\n","                imposed over the image described by the last 3\n","                channels\n","    \"\"\"\n","    assert len(images.shape) == 4\n","    num_images = images.shape[0]\n","    _, axes = plt.subplots(1, num_images, figsize=(28, 28))\n","    images = images.permute(0, 2, 3, 1).detach().numpy()\n","\n","    def handle_img(img, axe):\n","        axe.axis(\"off\")\n","        if img.shape[-1] == 1:\n","            img = (img - img.min())/(img.max() - img.min() + 1e-10)\n","            img = cv2.applyColorMap(np.uint8(img * 255), colormap=cv2.COLORMAP_JET)\n","        elif img.shape[-1] == 3:\n","            img = np.clip(denormalize(img), 0, 1)\n","        elif img.shape[-1] == 4:\n","            heatmap = img[..., 0]\n","            heatmap = (heatmap - heatmap.min())/(heatmap.max() - heatmap.min() + 1e-10)\n","            heatmap = cv2.applyColorMap(np.uint8(heatmap * 255), colormap=cv2.COLORMAP_JET)\n","            img = img[..., 1:]\n","            img = np.uint8(np.clip(denormalize(img), 0, 1) * 255)\n","            img = cv2.addWeighted(heatmap, 0.5, img, 0.5, 0)\n","\n","        axe.imshow(img)\n","\n","    if num_images == 0:\n","        handle_img(images[0], axes)\n","    else:\n","        for i, img in enumerate(images):\n","            handle_img(img, axes[i])\n","\n","\n","show_images(SAMPLE_ZEBRA)\n","show_images(SAMPLE_MAGPIE)"]},{"cell_type":"markdown","metadata":{"id":"BuKiJf6EQbRs"},"source":["# Grad-CAM"]},{"cell_type":"markdown","metadata":{"id":"uLvq_uugQbRs"},"source":["## The Model\n","First, we are going to download and inspect a model that we will try later to explain with Grad-CAM."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hHNWl8T0QbRs"},"outputs":[],"source":["# for showing the model structure\n","!pip install torchinfo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6CqDVVyYQbRs"},"outputs":[],"source":["import copy\n","import gc\n","\n","def create_fresh_model():\n","    model = copy.deepcopy(torchvision.models.vgg19(weights=\"DEFAULT\"))\n","    gc.collect()\n","    # later we are going to use a full backward hook\n","    # that won't work with in-place ops\n","    for m in model.features:\n","        if isinstance(m, torch.nn.ReLU):\n","            m.inplace = False\n","\n","    model.eval() # we disable dropout\n","\n","    for m in model.classifier:\n","        if isinstance(m, torch.nn.ReLU):\n","            m.inplace = False\n","\n","    return model\n","\n","\n","model = create_fresh_model()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CUR8fEDnQbRs"},"outputs":[],"source":["import torchinfo\n","torchinfo.summary(model, input_size=(1, 3, 224, 224), device=\"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0-8s26GcQbRs"},"outputs":[],"source":["model"]},{"cell_type":"markdown","metadata":{"id":"ih3mCbP7QbRs"},"source":["This model discriminates between 1000 classes. The class for zebra is 340, whereas the class for magpie is 18.  \n","If you are interested in other classes check this [link](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zAD7pH-vQbRs"},"outputs":[],"source":["ZEBRA_ID = 340\n","MAGPIE_ID = 18\n","\n","logits = model(SAMPLE_ZEBRA)\n","argmax = torch.argmax(logits, axis=-1)\n","print(f\"ZEBRA: {argmax}\")\n","\n","logits = model(SAMPLE_MAGPIE)\n","argmax = torch.argmax(logits, axis=-1)\n","print(f\"MAGPIE: {argmax}\")"]},{"cell_type":"markdown","metadata":{"id":"v3756UYtQbRs"},"source":["## The Grad-CAM\n","A brief description of Grad-CAM, a more detailed one can be found in [the paper in section 3 Grad-CAM](https://arxiv.org/abs/1610.02391).  \n","We omit the batch dimension for simplicity but your implementation should be able to handle it.  \n","\n","Let $I$ be the image of a zebra.  \n","Let $M$ be our model.   \n","Let $M(I)[z]$ be the logit corresponding to the zebra class - that is, for each image, our model outputs 1000 numbers that we will call logits.\n","The higher the i-th logit the more probable that the image belongs to the i-th class.  \n","Let $A$ be the **map activations** of some (usually last) convolution layer created when calling $M(I)$.\n","\n","We want to calculate $G[k, i, j] = \\frac{dM(I)[z]}{dA[k, i, j]}$.  \n","That is, we want to calculate the gradient of the logit with respect to each value from $A$.  \n","Note that as $A$ is the map activations of some convolution layer therefore it has channel, height, and width dimensions.  \n","\n","Now we average $G$ over spatial dimensions (height and width) creating $G'$, which we use to get the class-discriminative\n","localization map as follows:\n","\n","$L^c_{\\text{Grad-CAM}} = \\mathrm{ReLU}(\\sum_{k}{G'[k]A[k]})$\n","\n","We finally scale the result by putting the values in the range $[0, 1]$ and resize the result to match the image size using bilinear interpolation.\n","\n","Here your task is to finish the implementation of Grad-CAM.\n","\n","\n","Few hints:\n","* Let `x` be a tensor. If `x` is not a scalar then calling `x.backward()` will result in an error. That is because we should provide a starting gradient. For details, see [this doc](https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html).\n","* One can extract outputs and gradients of outputs of the layers using [forward](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=register_forward_hook#torch.nn.Module.register_forward_hook) and [backward](https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=register_full_backward#torch.nn.Module.register_full_backward_hook) hooks.\n","\n","\n","Below you can find examples of forward and backward hooks."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dXX6G3PQPjdZ"},"outputs":[],"source":["## EXAMPLE FORWARD HOOK\n","def forward_hook(module, input, output):\n","    altered_output = output * 2\n","    print(\n","        f\"\"\"Forward hook called\n","          module = {module}\n","          input = {input}\n","          output = {output}\n","          altered_output = {altered_output}\"\"\"\n","    )\n","    return altered_output\n","\n","\n","n = torch.nn.Sequential(torch.nn.Linear(10, 5), torch.nn.ReLU(), torch.nn.Linear(5, 1))\n","print(n)\n","n[1].register_forward_hook(forward_hook)\n","n[2].register_forward_hook(forward_hook)\n","\n","x = torch.randn((1, 10))\n","l = n(x)\n","print(l)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lEl0psRuPjdZ"},"outputs":[],"source":["## EXAMPLE BACKWARD HOOK\n","bakward_hook_on = True\n","\n","\n","def backward_hook(module, grad_input, grad_output):\n","    global bakward_hook_on\n","    altered_grad_input = (grad_input[0] * 10,) if bakward_hook_on else grad_input\n","    print(\n","        f\"\"\"Backward hook called\n","          module = {module}\n","          grad_input = {grad_input}\n","          grad_output = {grad_output}\n","          altered_grad_input = {altered_grad_input}\"\"\"\n","    )\n","    return altered_grad_input\n","\n","\n","n = torch.nn.Sequential(torch.nn.Linear(10, 5), torch.nn.ReLU(), torch.nn.Linear(5, 1))\n","print(n)\n","n[0].register_full_backward_hook(backward_hook)\n","\n","x = torch.randn((1, 10))\n","x.requires_grad = True\n","l = n(x)\n","l.backward()\n","print(f\"x.grad {x.grad}\")\n","\n","bakward_hook_on = False\n","x.grad = None\n","l = n(x)\n","l.backward()\n","print(f\"x.grad {x.grad}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c9CMSbyAQbRs"},"outputs":[],"source":["class ModelWithGradCam:\n","    def __init__(self, model_creator, layer_id):\n","        \"\"\"\n","        Args:\n","          model_creator - a function that creates a model upon a call\n","          layer_id - id of the layer whose output we are paying attention to\n","        \"\"\"\n","        self.model: torch.nn.Module = model_creator()\n","        self.model_layer: torch.nn.Module = self.model.features[\n","            layer_id\n","        ]  # the layer whose output we are paying attention to\n","        self.forward_pass = None  # the result of the forward pass on self.model_layer (A in the description above)\n","        self.grad_pass = None  # the gradient with respect to self.forward_pass (G in the description above)\n","\n","        self.register_hooks()\n","\n","    def register_hooks(self):\n","        \"\"\"\n","        Registers hooks for getting the output of self.model_layer\n","        and the gradient with respect to this output\n","        \"\"\"\n","\n","        # HINT:\n","        # https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=register_forward_hook#torch.nn.Module.register_forward_hook\n","        # https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=register_full_backward#torch.nn.Module.register_full_backward_hook\n","        # Note be careful, inspect forward and backward hook inputs\n","        ## TODO {\n","\n","        ## }\n","\n","    def get_grad_cam(self, input: torch.Tensor, class_id: int) -> torch.Tensor:\n","        \"\"\"\n","        For a given input of shape [BATCH, C, H, W]\n","        calculates the class-discriminative\n","        localization map of shape [BATCH, 1, H, W]\n","        (one for each element of the batch) for class class_id\n","        \"\"\"\n","        self.model.zero_grad()\n","\n","        ## TODO {\n","\n","\n","        ## }\n","\n","        assert self.forward_pass.shape == self.grad_pass.shape\n","        assert len(self.forward_pass.shape) == 4  # [BATCH, C', H', W']\n","        assert len(self.grad_pass.shape) == 4\n","\n","        assert gcam.shape[0] == input.shape[0]\n","        assert gcam.shape[1] == 1\n","        assert gcam.shape[2:] == input.shape[2:]\n","\n","        return gcam"]},{"cell_type":"markdown","metadata":{"id":"mY_voZSzQbRt"},"source":["Let's test the implementation. You can experiment with different layer_id (pay attention to the brief description above). In general, deeper layers should contain more high-level information."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gfxEDBkBQbRt"},"outputs":[],"source":["model_with_cam = ModelWithGradCam(create_fresh_model, 35)\n","try:\n","    gcam = model_with_cam.get_grad_cam(SAMPLE_ZEBRA, ZEBRA_ID)\n","finally:\n","    del model_with_cam\n","    gc.collect()\n","show_images(torch.concat([gcam, SAMPLE_ZEBRA], dim=-3))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZCwHnQKZQbRt"},"outputs":[],"source":["model_with_cam = ModelWithGradCam(create_fresh_model, 35)\n","try:\n","    gcam = model_with_cam.get_grad_cam(SAMPLE_MAGPIE, MAGPIE_ID)\n","finally:\n","    del model_with_cam\n","    gc.collect()\n","show_images(torch.concat([gcam, SAMPLE_MAGPIE], dim=-3))"]},{"cell_type":"markdown","metadata":{"id":"gmdcpHZSQbRt"},"source":["## Guided Grad-Cam\n","Finish implementation of guided Grad-CAM below.  \n","The brief explanation behind this method is presented in  [the paper section 3.2 Guided Grad-CAM](https://arxiv.org/abs/1610.02391).   \n","The main idea is to additionally calculate the gradient of class logit with respect to each pixel of the input image, but during this computation\n","we change negative gradients with respect to ReLU inputs to zero.    \n","In the end, we take image gradients and multiply them point-wise by the class-discriminative\n","localization map calculated by Grad-CAM.  \n","\n","Few hints:\n","* `input.requires_grad = True`\n","* `x.detach()`\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"77Sq-LIjQbRt"},"outputs":[],"source":["class ModelWithGuidedGradCam:\n","    def __init__(self, model_creator, layer_id):\n","        \"\"\"\n","        Args:\n","          model_creator - a function that creates a model upon a call\n","          layer_id - id of the layer whose output we are paying attention to\n","        \"\"\"\n","        self.model = model_creator()\n","        self.model_with_gc = ModelWithGradCam(model_creator, layer_id)\n","        self.register_hooks()\n","\n","    def register_hooks(self):\n","        \"\"\"\n","        Registers hook for each ReLU in the model\n","        for updating the gradient with respect to\n","        the ReLU input\n","        \"\"\"\n","\n","        def modify_backward(module, i, o):\n","            ## TODO {\n","\n","            ## }\n","\n","        for module in self.model.features:\n","            module: torch.nn.Module\n","            if isinstance(module, torch.nn.ReLU):\n","                module.register_full_backward_hook(modify_backward)\n","\n","        for module in self.model.classifier:\n","            module: torch.nn.Module\n","            if isinstance(module, torch.nn.ReLU):\n","                module.register_full_backward_hook(modify_backward)\n","\n","    def get_guided_grad_cam(self, input, class_id, use_grad_cam=True):\n","        gcam = self.model_with_gc.get_grad_cam(input, class_id)\n","\n","        self.model.zero_grad()\n","        ## TODO {\n","\n","        ## }\n","        assert img_grad.shape == input.shape\n","        assert len(img_grad.shape) == len(gcam.shape)\n","        if use_grad_cam:\n","            res = img_grad * gcam\n","        else:\n","            res = img_grad  # gradient of class logit with respect to the input image pixels\n","\n","        return res"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TTD5mcB-QbRt"},"outputs":[],"source":["model_with_ggcam = ModelWithGuidedGradCam(create_fresh_model, 35)\n","grad = model_with_ggcam.get_guided_grad_cam(\n","    SAMPLE_ZEBRA, ZEBRA_ID, use_grad_cam=False\n",")  # first without Grad-CAM\n","try:\n","    gcam = model_with_ggcam.get_guided_grad_cam(SAMPLE_ZEBRA, ZEBRA_ID)\n","finally:\n","    del model_with_ggcam\n","    gc.collect()\n","show_images(grad)\n","show_images(gcam)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cKUKHN75QbRt"},"outputs":[],"source":["model_with_ggcam = ModelWithGuidedGradCam(create_fresh_model, 35)\n","grad = model_with_ggcam.get_guided_grad_cam(\n","    SAMPLE_MAGPIE, MAGPIE_ID, use_grad_cam=False\n",")\n","try:\n","    gcam = model_with_ggcam.get_guided_grad_cam(SAMPLE_MAGPIE, MAGPIE_ID)\n","finally:\n","    del model_with_ggcam\n","    gc.collect()\n","\n","show_images(grad)\n","show_images(gcam)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}